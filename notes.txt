Avg Precision:

CP - 1
recommendations = [0,1,1,0,1,1,0]
precisions = [0/1,1/2,2/3,2/4,3/5,4/6,4/7] = [0,0.5,0.67,0.5,0.6,0.67,0.7142]
avg_precision = 3.6542/7 = 0.5220

CP -2
recommendations = [1,0,1,0,1,0,1]
precisions = [1/1,1/2,2/3,2/4,3/5,3/6,4/7] = [1,0.5,0.67,0.5,0.6,0.5,0.71]
avg_precision = 4.48/7 = 0.64

Mean Avg Precision = 4.48 + 3.654 / 2 =   8.134/2 = 4.067


* update tfidf scores in google sheets
* rerun glove, bert and bert-1 and update scores in google sheets
* pertubation analysis 
* run the notebooks for other bert layers


TO-DO: Completed
-----------------
* Clustering analysis , are the clusters making sense ? (are they relevant to politics ?) - Done
* Word Overlap from above - Done
* Use Topic Classified Articles, use the topics as clusters - Done
* What are the top overlapping terms ? - Done
* Top coefficients from these classifiers - Done
* Word Similarity vs Bert Layer - Done

To-Do:
------
* Redo code so we can run baselines without graph creation and with score retention
* Run baselines for new data with scaling
* Run baselines for bert with no context and with context for only layer 12 with scaling
* How do we vary contextualization within a given layer
    * we know how to do it in the extreme case
    * if we want a knob effect:
        * sample a percentage of words and decontextualize
        * sample a percentage of words and decontextualize only those reps