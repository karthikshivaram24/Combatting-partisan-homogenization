{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....... Initializing Settings ..... \n",
      "Random_Seed Chosen : 15112\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from bert_utils import load_tokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from preprocess_utils import preprocess_texts\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import itertools\n",
    "from bert_embeddings import load_bert_embeddings\n",
    "from clustering_utils import run_clustering, get_cluster_sizes, score_cluster, get_cluster_pairs, get_pairwise_dist, cluster2doc, filter_clusters, get_top_100_clusterpairs\n",
    "import config as CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "-------\n",
    "* Reasons a document may not get a context word when sampling\n",
    "    * IDF thresh\n",
    "    * Unkown (missing from vocabulary)\n",
    "    * when tokenized all are word pieces (if not observing word pieces)\n",
    "    * Contains only stopwords and numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still Working On :\n",
    "-------------------\n",
    "* Flip labels for heterogeneous - **done**\n",
    "* Replace tokens for pos samples - **done**\n",
    "* Batch size > 2\n",
    "* Iterative training\n",
    "* cross val for parameter choosing\n",
    "* split train for val set \n",
    "* freeze bert layer 12 option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow :\n",
    "------\n",
    "\n",
    "* Dataset Prep\n",
    "    * Preprocess\n",
    "    * context word generation\n",
    "* Bert Embeddings For Clustering\n",
    "* Clustering\n",
    "* Picking Cluster Pairs\n",
    "* For each Cluster Pair :\n",
    "    * Training\n",
    "    * Metric Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../sampled_articles_from_relevant_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_b = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in test_b.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_utils\n",
    "\n",
    "# Need:\n",
    "# 1) A way to generate vocabulary to sample from\n",
    "# 2) Sampling Context Words\n",
    "# 3) Negative Binary Sampling to generate negative labels for the context words\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    tokenizer = load_tokenizer()\n",
    "    df = pd.read_csv(data_path)\n",
    "    df.drop(columns=[\"all_text\"],inplace=True)\n",
    "    df[\"processed_title\"] = preprocess_texts(df[\"title\"].tolist())\n",
    "    return df\n",
    "    \n",
    "\n",
    "def calc_idf(vocab_dict,num_docs = 100000):\n",
    "    \"\"\"\n",
    "    idf_t = log(N/df_t)\n",
    "    \n",
    "    Large it is rarer the term, smaller it is more frequent the term\n",
    "    \"\"\"\n",
    "    return {key: np.log2(num_docs/vocab_dict[key]) for key in vocab_dict.keys()}\n",
    "\n",
    "def filter_vocab(vocab_idf_dict,thresh=10.0,filter_word_pieces=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # remove words from title vocab that have very low idf values and are stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    vocab_idf = dict(filter(lambda x: x[1]>=thresh, vocab_idf_dict.items()))\n",
    "    vocab_idf = dict(filter(lambda x: x[0] not in stop_words, vocab_idf.items()))\n",
    "    vocab_idf = dict(filter(lambda x: \"UNK\" not in x[0], vocab_idf.items())) # Token absent from bert's vocab\n",
    "    vocab_idf = dict(filter(lambda x: not x[0].isnumeric(), vocab_idf.items())) # Don't include numbers to represent context words\n",
    "    vocab_idf = dict(filter(lambda x: len(x[0]) >= 3,vocab_idf.items())) # filter out small words\n",
    "    \n",
    "    if filter_word_pieces:\n",
    "        vocab_idf = dict(filter(lambda x: \"##\" not in x[0], vocab_idf.items()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return vocab_idf\n",
    "\n",
    "def get_vocab(text_list):\n",
    "    \"\"\"\n",
    "    Here we get the vocab to sample from for obtaining our context word also has idf information\n",
    "    \n",
    "    Strategy 1: Using words from the title\n",
    "    \"\"\"\n",
    "    tokens_list = []\n",
    "    tokenizer = load_tokenizer()\n",
    "    vocab_df = defaultdict(int)\n",
    "    for text in text_list:\n",
    "        token_ids = tokenizer.encode(text,add_special_tokens=False)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        \n",
    "        for token in tokens:\n",
    "            vocab_df[token] +=1\n",
    "        \n",
    "    vocab_idf = calc_idf(vocab_df)\n",
    "    \n",
    "    print(\"Vocab_size : %s\" %str(len(vocab_idf.keys())))\n",
    "    \n",
    "    return vocab_idf\n",
    "\n",
    "def replace_context_word(token_text,context_word):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    token_text.remove(context_word)\n",
    "    \n",
    "    return token_text\n",
    "\n",
    "def sample_context_words(text_list,idf_thresh=10.0,pos_sample_num=3,neg_sample_num=3,cword_type=\"pos\",filter_word_pieces=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    vocab_idf = get_vocab(text_list)\n",
    "    vocab_idf = filter_vocab(vocab_idf,\n",
    "                             thresh=idf_thresh,\n",
    "                             filter_word_pieces=filter_word_pieces)\n",
    "    \n",
    "    print(\"Vocab Size After Filtering : %s\" %str(len(vocab_idf.keys())))\n",
    "    tokenizer = load_tokenizer()\n",
    "    \n",
    "    if cword_type == \"pos\":\n",
    "        \n",
    "        context_words = []\n",
    "        fails = 0\n",
    "        for text in text_list :\n",
    "            context_word = sample_context_words_pos(text,vocab_idf,tokenizer,filter_word_pieces=filter_word_pieces,pos_sample_num=pos_sample_num)\n",
    "            if \"DROP_THIS\" in Counter(context_word) and Counter(context_word)[\"DROP_THIS\"] == pos_sample_num:\n",
    "                fails+=1\n",
    "            context_words.append(context_word)\n",
    "        \n",
    "        print(\"Failed to find context words for : %s docs\" %str(fails))\n",
    "        return context_words\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        context_words = []\n",
    "        fails= 0\n",
    "        for text in text_list :\n",
    "            # result is list of lists where list[0] has length = neg_sample_num\n",
    "            con_wl = sample_context_words_neg(text,vocab_idf,tokenizer,neg_sample_num=neg_sample_num)\n",
    "            if con_wl == None:\n",
    "                fails+=1\n",
    "            context_words.append(con_wl)\n",
    "            \n",
    "        print(\"Failed to find context words for : %s docs\" %str(fails))\n",
    "        return context_words\n",
    "\n",
    "def sample_context_words_pos(text,vocab_idf,tokenizer,pos_sample_num=3,filter_word_pieces=True):\n",
    "    \"\"\"\n",
    "    Sample from titles vocab but don't include stopwords or low-idf terms (frequent terms)\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text,add_special_tokens=False))\n",
    "    \n",
    "    if filter_word_pieces:\n",
    "        tokens = [t for t in tokens if \"#\" not in t]\n",
    "    \n",
    "    sampled_indices = random.sample(range(len(tokens)),k=len(tokens))\n",
    "    context_word = None\n",
    "    loop_counter = 0\n",
    "    temp_c = 0\n",
    "    context_words = []\n",
    "    for indice in sampled_indices:\n",
    "        if temp_c == pos_sample_num:\n",
    "            break\n",
    "        context_word = tokens[sampled_indices[indice]]\n",
    "        if context_word in vocab_idf:\n",
    "            context_words.append(context_word)\n",
    "            context_word = None\n",
    "            temp_c +=1\n",
    "            \n",
    "    # Nones to add:\n",
    "    nones_to_add = pos_sample_num - len(context_words)\n",
    "    if nones_to_add > 0:\n",
    "        context_words += [\"DROP_THIS\"] * nones_to_add\n",
    "    \n",
    "    return context_words\n",
    "\n",
    "\n",
    "def sample_context_words_neg(text,vocab_idf,tokenizer,neg_sample_num=3):\n",
    "    \"\"\"\n",
    "    Similar to Pos context word sampling \n",
    "    \n",
    "    sample from titles vocab but don't include stopwords or low-idf terms (frequent terms) and not in text\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text,add_special_tokens=False))\n",
    "    \n",
    "    context_words = None\n",
    "    hard_stop = 0\n",
    "    while True:\n",
    "        context_words = random.sample(list(vocab_idf.keys()),k=neg_sample_num)\n",
    "        hard_stop += 1\n",
    "        not_in_vocab = 0\n",
    "        for c_w in context_words:\n",
    "            if c_w not in tokens:\n",
    "                not_in_vocab +=1\n",
    "        \n",
    "        if not_in_vocab == neg_sample_num:\n",
    "            break\n",
    "        \n",
    "        if hard_stop == 11:\n",
    "            context_words = None\n",
    "            break\n",
    "    \n",
    "    return context_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_text_gen(input_1_list,\n",
    "                   input_2_list,\n",
    "                   output_1_list,\n",
    "                   output_2_list,batch_size=500):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for ndx in range(0,len(input_1_list),batch_size):\n",
    "        yield input_1_list[ndx:min(ndx+batch_size,len(input_1_list))], input_2_list[ndx:min(ndx+batch_size,len(input_2_list))], output_1_list[ndx:min(ndx+batch_size,len(output_1_list))], output_2_list[ndx:min(ndx+batch_size,len(output_2_list))]\n",
    "\n",
    "def tokenize_4bert_batch(input1,input2,output1,output2,tokenizer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cuda1 = torch.device('cuda:1')\n",
    "    tokenized_tensor = torch.LongTensor([tokenizer.encode(text,\n",
    "                                                          truncation=True,\n",
    "                                                          padding=\"max_length\",\n",
    "                                                          max_length=500, \n",
    "                                                          add_special_tokens=True)\n",
    "                                                          for text in input1])\n",
    "    \n",
    "    tokenized_context_word = torch.LongTensor([tokenizer.encode(word,truncation=True,add_special_tokens=False,max_length=1) for word in input2])\n",
    "    \n",
    "    class_labels = torch.FloatTensor(output1)\n",
    "    word_labels = torch.FloatTensor(output2)\n",
    "    \n",
    "    tokenized_tensor = tokenized_tensor.to(cuda1)\n",
    "    tokenized_context_word = tokenized_context_word.to(cuda1)\n",
    "    class_labels = class_labels.to(cuda1)\n",
    "    word_labels = word_labels.to(cuda1)\n",
    "    \n",
    "    return tokenized_tensor, tokenized_context_word, class_labels, word_labels\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "class AttentionMT(nn.Module):\n",
    "    \n",
    "    def __init__(self,embedding_size=768,verbose=True,bert_train=False):\n",
    "        super(AttentionMT,self).__init__()\n",
    "        \n",
    "        cuda1 = torch.device('cuda:1')\n",
    "        self.verbose = verbose\n",
    "        self.bert_train = bert_train\n",
    "        # Embedding Layer (Here we are using pre-trained embeddings hence using the Bert Model)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "        self.bert.to(cuda1)\n",
    "        if self.bert_train:\n",
    "            # Set the pre-trained model in training mode (for Fine Tuning)\n",
    "            self.bert.train()\n",
    "        if not self.bert_train:\n",
    "            # Set the pre-trained model in evaluation mode (for inference only)\n",
    "            self.bert.eval()\n",
    "        \n",
    "        #Attention Layer\n",
    "        # -----------------\n",
    "        # This attention is applied to individual word_embeddings\n",
    "        # Input to this layer : (embedding_size)\n",
    "        # weight Matrix = (in_feats,out_feats) (768,1)\n",
    "        # output will have shape = (batch_size,1)\n",
    "        \n",
    "        self.attention = nn.Linear(in_features=embedding_size,\n",
    "                                   out_features=1,\n",
    "                                   bias=False)\n",
    "        \n",
    "        # Recommendation Network\n",
    "        # ------------------------\n",
    "        \n",
    "        self.recom_pred = nn.Linear(in_features=embedding_size,\n",
    "                                    out_features=1,\n",
    "                                    bias=True)\n",
    "        \n",
    "        # Softmax Activation\n",
    "        # dim -1 for last dimension\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # Sigmoid Activation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,\n",
    "                bert_tokenized_words,\n",
    "                bert_tokenized_word_to_predict):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        ------\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        \"\"\"\n",
    "        \n",
    "        # ************ Recommendation Network ********************\n",
    "        \n",
    "        # output shape should be (max_length,dim)\n",
    "        bert_output = self.bert(input_ids=bert_tokenized_words)\n",
    "        bert_hidden_states  = bert_output[2]\n",
    "        bert_layer12_hidden_states = bert_hidden_states[-1][0,:,:]\n",
    "       \n",
    "        \n",
    "        # we calculate attention\n",
    "        attention_un = torch.cat([self.attention(embedding) for embedding in bert_layer12_hidden_states])  # shape (N,max_length,1) *** u_it ***\n",
    "        \n",
    "#         atten_un_batch = []\n",
    "#         for i in range(bert_layer12_hidden_states.size(0)):\n",
    "#             atten_un_single = []\n",
    "#             for j in range(bert_layer12_hidden_states.size(1)):\n",
    "#                 atten_un_single.append(self.attention(bert_layer12_hidden_states[i,j,:]))\n",
    "            \n",
    "#             atten_un_batch.append(torch.cat(atten_un_single))\n",
    "#         atten_un_batch = torch.cat(atten_un_batch,dim=0)\n",
    "        \n",
    "        # (batch_size,max_length,embedding_size) * (embedding_size,1) = (batch_size,max_length)\n",
    "        \n",
    "#         attention_un = self.attention(bert_layer12_hidden_states)\n",
    "        \n",
    "        attentions = self.softmax(attention_un) # shape (N,max_length,1) ***  a_it ***\n",
    "     \n",
    "        attention_cvector = bert_layer12_hidden_states.T.mul(attentions).sum(dim=1) # shape (N,768,1) *** s_i ***\n",
    "        \n",
    "        # predict if user likes / dislikes article (figure out how to get predicted probabilities)\n",
    "        y_pred = self.sigmoid(self.recom_pred(attention_cvector.T))\n",
    "        \n",
    "        # ************** Word Prediction Network ********************\n",
    "        # predict if context word is present in the sentence\n",
    "        context_word_embed_bert = self.bert(input_ids=bert_tokenized_word_to_predict)\n",
    "        context_word_embed_bert_hs = context_word_embed_bert[2]\n",
    "        context_word_embed_bert_layer_12 = context_word_embed_bert_hs[-1][0,:,:]\n",
    "        \n",
    "        # Single Neuron / LR prediction\n",
    "        context_pred = self.sigmoid(torch.mul(attention_cvector,context_word_embed_bert_layer_12).sum(dim=1))\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\nShape Details :\")\n",
    "            print(\"1. Bert Embeddings Shape : %s\" %str(bert_layer12_hidden_states.size()))\n",
    "            print(\"2. attention_un Shape : %s\" %str(attention_un.size()))\n",
    "            print(\"3. attention_norm Shape : %s\" %str(attentions.size()))\n",
    "            print(\"4. attention_cvector shape : %s\" %str(attention_cvector.size()))\n",
    "            print(\"5. y_pred shape : %s\" %str(y_pred.size()))\n",
    "            print(str(y_pred.item()))\n",
    "            print(\"6. context_word_embed_bert shape : %s\" %str(context_word_embed_bert_layer_12.size()))\n",
    "            print(\"7. context_pred shape : %s\" %str(context_pred.size()))\n",
    "            print(str(context_pred.item()))\n",
    "        \n",
    "        return y_pred, context_pred, attention_cvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering utils\n",
    "\n",
    "def get_cluster_pairs_top(df,vectors,sample_size=3):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Now drop the rows with Nones for context_word_pos and similarly do the same to the embeddings\n",
    "    \n",
    "    df[\"drop_\"] = df['context_word_pos'].apply(lambda x: x == [\"DROP_THIS\"]*sample_size)\n",
    "    \n",
    "    indices_to_drop = df.index[df['drop_'] == True].tolist()\n",
    "    print(\"Indices to drop : %s\" %str(len(indices_to_drop)))\n",
    "    clean_df = df.drop(df.index[indices_to_drop])\n",
    "    clean_df.reset_index(drop=True,inplace=True)\n",
    "    print(clean_df.head(2))\n",
    "    print(\"New Df Shape : %s\" %str(clean_df.shape))\n",
    "    \n",
    "    vectors = np.delete(vectors,indices_to_drop,axis=0)\n",
    "    \n",
    "    clusters,cluster_clf = run_clustering(vectors=vectors,\n",
    "                                              seed=CONFIG.RANDOM_SEED,\n",
    "                                              num_clusters=CONFIG.num_clusters,\n",
    "                                              clus_type=\"kmeans\")\n",
    "    \n",
    "    doc_2_cluster_map = cluster2doc(num_texts= clean_df.shape[0],\n",
    "                                    cluster_labels=cluster_clf.labels_)\n",
    "    \n",
    "    \n",
    "    cluster_sizes = get_cluster_sizes(cluster_clf)\n",
    "    \n",
    "    cluster_pairs = get_cluster_pairs(num_clusters=CONFIG.num_clusters)\n",
    "    \n",
    "    filtered_cluster_pairs = filter_clusters(cluster_pairs=cluster_pairs,\n",
    "                                            doc_2_cluster_map=doc_2_cluster_map,\n",
    "                                            cluster_sizes=cluster_sizes,\n",
    "                                            partisan_scores=clean_df[\"binary_ps\"].tolist(),\n",
    "                                            min_size=CONFIG.min_cluster_size,\n",
    "                                            max_size=CONFIG.max_cluster_size,\n",
    "                                            min_partisan_size=CONFIG.min_partisan_size)\n",
    "    \n",
    "    top100 = None\n",
    "    \n",
    "    if len(filtered_cluster_pairs) > 100:\n",
    "        print(\"\\nNumber of Filtered Cluster Pairs are greater 100, picking top 100 most similar cluster pairs\")\n",
    "        cluster_pair_dist_mat = get_pairwise_dist(cluster_clf,dist_type=\"cosine\")\n",
    "        top100 = get_top_100_clusterpairs(cluster_pairs=filtered_cluster_pairs,dist_matrix=cluster_pair_dist_mat,reverse=True)\n",
    "        \n",
    "    else:\n",
    "        top100 = filtered_cluster_pairs\n",
    "        print(\"\\nNumber of Filtered Cluster Pairs is less than 100 so skipping top 100 selection\")\n",
    "    \n",
    "    return clean_df , doc_2_cluster_map, top100 \n",
    "\n",
    "# mocking baseline 2\n",
    "\n",
    "def run_all_cps(df,cps,doc_2_cluster_map,learning_rate=0.01,context_pred_loss_weight=0.5,num_epochs=2,batch_size=1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # For every cp\n",
    "    # select relevant rows\n",
    "    # create train - cluster 1, test - cluster2\n",
    "    # train model\n",
    "    # record loss\n",
    "    # incremental train on test\n",
    "    # check metrics\n",
    "    \n",
    "    cp_scores = defaultdict(lambda : defaultdict(list))\n",
    "    epoch_train_loss_cps = defaultdict()\n",
    "    inc_test_loss_cps = defaultdict()\n",
    "    for cp in cps:\n",
    "        print(\"cluster_pair : %s\" %str(cp))\n",
    "        train,test = get_train_test_attm(df,cp,doc_2_cluster_map)\n",
    "        \n",
    "        print(train.head(10))\n",
    "        model, epoch_losses, opt, loss_func = train_model(train,\n",
    "                                                          learning_rate=learning_rate,\n",
    "                                                          context_pred_loss_weight=context_pred_loss_weight,\n",
    "                                                          num_epochs=num_epochs,\n",
    "                                                          batch_size=batch_size)\n",
    "        \n",
    "#         inc_losses_test = incremental_train(model=model,\n",
    "#                                               data=test,\n",
    "#                                               opt=opt,\n",
    "#                                               loss_func=loss_func,\n",
    "#                                               context_pred_loss_weight=context_pred_loss_weight,\n",
    "#                                               increments=100)\n",
    "        \n",
    "#         cp_scores[cp][\"train_avg_presc\"] = \n",
    "#         cp_scores[cp][\"train_loss\"] = \n",
    "#         cp_scores[cp][\"test_avg_presc\"] =\n",
    "#         cp_scores[cp][\"test_loss\"] = \n",
    "        epoch_train_loss_cps[cp]= epoch_losses\n",
    "#         inc_test_loss_cps[cp] = inc_losses_test\n",
    "        \n",
    "    \n",
    "    return epoch_train_loss_cps, inc_test_loss_cps\n",
    "\n",
    "def gen_samples(df,neg_sample_size=3):\n",
    "    \"\"\"\n",
    "    columns = processed_text,processed_title, context_word_pos, context_word_neg\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"processed_all\"] = df[\"processed_title\"] + \" \" + df[\"processed_text\"]\n",
    "    \n",
    "    text_list = df[\"processed_all\"].tolist()\n",
    "    ps_labels = df[\"binary_ps\"].tolist()\n",
    "    pos_con_word = df[\"context_word_pos\"].tolist()\n",
    "    neg_con_word = df[\"context_word_neg\"].tolist()\n",
    "    \n",
    "    text_list_neg = []\n",
    "    ps_labels_neg = []\n",
    "    text_list_pos = []\n",
    "    ps_labels_pos = []\n",
    "    \n",
    "    for ind_t, text in enumerate(text_list):\n",
    "        text_list_neg.append([text]*neg_sample_size)\n",
    "        ps_labels_neg.append([ps_labels[ind_t]]*neg_sample_size)\n",
    "        text_list_pos.append([text]*neg_sample_size)\n",
    "        ps_labels_pos.append([ps_labels[ind_t]]*neg_sample_size)\n",
    "        \n",
    "    text_list_neg = list(itertools.chain(*text_list_neg))\n",
    "    neg_con_word = list(itertools.chain(*neg_con_word))\n",
    "    ps_labels_neg = list(itertools.chain(*ps_labels_neg))\n",
    "    \n",
    "    text_list_pos = list(itertools.chain(*text_list_pos))\n",
    "    pos_con_word = list(itertools.chain(*pos_con_word))\n",
    "    ps_labels_pos = list(itertools.chain(*ps_labels_pos))\n",
    "    \n",
    "    assert len(text_list_neg) == len(neg_con_word)\n",
    "    assert len(text_list_neg) == len(text_list_pos)\n",
    "    assert len(text_list_pos) == len(pos_con_word)\n",
    "    \n",
    "    all_text_list = text_list_pos + text_list_neg\n",
    "    all_con_word = pos_con_word + neg_con_word\n",
    "    all_word_labels = ([1]*len(pos_con_word)) + ([0] * len(neg_con_word))\n",
    "    all_ps_labels = ps_labels_pos + ps_labels_neg\n",
    "    \n",
    "    df_sample = pd.DataFrame()\n",
    "    df_sample[\"text\"] = all_text_list\n",
    "    df_sample[\"context_word\"] = all_con_word\n",
    "    df_sample[\"word_label\"] = all_word_labels\n",
    "    df_sample[\"class_label\"] = all_ps_labels\n",
    "    \n",
    "    df_sample = df_sample.loc[df_sample[\"context_word\"] != \"DROP_THIS\"].reset_index(drop=True)\n",
    "    \n",
    "    # Shuffle twice\n",
    "    df_sample = df_sample.sample(frac=1.0,random_state=CONFIG.RANDOM_SEED)\n",
    "    df_sample = df_sample.sample(frac=1.0,random_state=CONFIG.RANDOM_SEED+1)\n",
    "    \n",
    "    return df_sample\n",
    "    \n",
    "    \n",
    "\n",
    "def get_train_test_attm(df,cp,doc_2_cluster_map,neg_sample_size=3):\n",
    "    \"\"\"\n",
    "    train and test we need to first subsample using doc_2_cluster_map for each cluster\n",
    "    get train and test , then create negative samples and finally shuffle both train and test\n",
    "    \"\"\"\n",
    "    cluster1_indices = doc_2_cluster_map[cp[0]]\n",
    "    cluster2_indices = doc_2_cluster_map[cp[1]]\n",
    "    \n",
    "    train_df = df.iloc[cluster1_indices].reset_index(drop=True)\n",
    "    test_df = df.iloc[cluster2_indices].reset_index(drop=True)\n",
    "    \n",
    "    print(\"Train Shape : %s\" %str(train_df.shape))\n",
    "    print(\"Test Shape : %s\" %str(test_df.shape))\n",
    "    \n",
    "    train_df = gen_samples(train_df,neg_sample_size=neg_sample_size)\n",
    "    test_df = gen_samples(test_df,neg_sample_size=neg_sample_size)\n",
    "    \n",
    "    print(\"Original class_labels : \\n%s\" %str(test_df[\"class_label\"].iloc[:10]))\n",
    "    \n",
    "    test_df[\"class_label\"] = test_df[\"class_label\"].apply(lambda x: np.abs(x+(-1)))\n",
    "    \n",
    "    print(\"Flipped class_labels : \\n%s\" %str(test_df[\"class_label\"].iloc[:10]))\n",
    "    \n",
    "    print(\"Train Shape : %s\" %str(train_df.shape))\n",
    "    print(\"Test Shape : %s\" %str(test_df.shape))\n",
    "    \n",
    "    train_df = train_df.sample(frac=1,random_state=CONFIG.RANDOM_SEED).reset_index(drop=True)\n",
    "    test_df = test_df.sample(frac=1,random_state=CONFIG.RANDOM_SEED).reset_index(drop=True)\n",
    "    \n",
    "    print(\"Train Shape : %s\" %str(train_df.shape))\n",
    "    print(\"Test Shape : %s\" %str(test_df.shape))\n",
    "    return train_df,test_df\n",
    "\n",
    "def train_model(data,learning_rate=0.01,context_pred_loss_weight=0.5,num_epochs=2,batch_size=1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cuda1 = torch.device('cuda:1')\n",
    "    model = AttentionMT(embedding_size=768,verbose=False)\n",
    "    model.to(cuda1)\n",
    "    loss_func = nn.BCELoss()\n",
    "    opt = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "    bert_tokenizer = load_tokenizer()\n",
    "    \n",
    "    epoch_losses = defaultdict(lambda : defaultdict(list))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "#         print(\"\\n ****** Epoch %s ********\" %str(epoch))\n",
    "        batch_nums = data.shape[0]/batch_size\n",
    "        \n",
    "        batch_total_loss = []\n",
    "        batch_word_loss = []\n",
    "        batch_rs_loss = []\n",
    "        \n",
    "        articles = data[\"text\"].tolist()\n",
    "        context_words = data[\"context_word\"].tolist()\n",
    "        class_labels = data[\"class_label\"].tolist()\n",
    "        word_labels = data[\"word_label\"].tolist()\n",
    "        \n",
    "        for batch_num, (article_batch, context_word_batch, class_label_batch, word_label_batch) in enumerate(batch_text_gen(articles,\n",
    "                                                                                                                             context_words,\n",
    "                                                                                                                             class_labels,\n",
    "                                                                                                                             word_labels,\n",
    "                                                                                                                            batch_size=batch_size)):\n",
    "#             print(\"\\n ----- Batch %s -----\"%str(batch_num))\n",
    "            \n",
    "            bert_tokenized_words, bert_tokenized_word_to_predict, rec_labels, word_labels = tokenize_4bert_batch(article_batch,\n",
    "                                                                                                                 context_word_batch, \n",
    "                                                                                                                 class_label_batch, \n",
    "                                                                                                                 word_label_batch, tokenizer=bert_tokenizer)\n",
    "            opt.zero_grad() # reset all the gradient information\n",
    "    \n",
    "            y_pred, context_pred, attention_vector = model(bert_tokenized_words, bert_tokenized_word_to_predict)\n",
    "            \n",
    "            rec_loss = loss_func(y_pred,rec_labels)\n",
    "            word_loss = loss_func(context_pred,word_labels)\n",
    "            \n",
    "            total_loss = rec_loss + (context_pred_loss_weight * word_loss)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            \n",
    "            batch_rs_loss.append(rec_loss.item())\n",
    "            batch_word_loss.append(word_loss.item())\n",
    "            batch_total_loss.append(total_loss)\n",
    "            \n",
    "            if batch_num % 100 == 0 and batch_num >=100:\n",
    "                print(\"Epoch : %s | Batch : %s | Total Loss : %s | Rec Loss : %s | Word Loss : %s\" % (str(epoch),str(batch_num),str(total_loss.item()),str(rec_loss.item()),str(word_loss.item())))\n",
    "            \n",
    "            \n",
    "        epoch_losses[epoch][\"rs_loss\"].append(batch_rs_loss)\n",
    "        epoch_losses[epoch][\"word_loss\"].append(batch_word_loss)\n",
    "        epoch_losses[epoch][\"total_loss\"].append(batch_total_loss)\n",
    "    \n",
    "    return model, epoch_losses, opt, loss_func\n",
    "\n",
    "def partial_train_step(x1,x2,model,opt,loss_func,context_pred_loss_weight=0.5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    opt.zero_grad()\n",
    "    y_pred, context_pred, attention_vector = model(x1, x2)\n",
    "    rec_loss = loss_func(y_pred,rec_labels)\n",
    "    word_loss = loss_func(context_pred,word_labels)\n",
    "    total_loss = rec_loss + (context_pred_loss_weight * word_loss)\n",
    "    total_loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    return y_pred, context_pred, attention_vector, rec_loss, word_loss, total_loss\n",
    "\n",
    "def plot_epoch_loss(epoch_losses):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    fig,ax = plt.figure(figsize=(10,10))\n",
    "    ax.plot(range(epoch_losses[0][\"total_loss\"]),epoch_losses[0][\"total_loss\"],c=\"green\",label=\"Total Loss\")\n",
    "    ax.plot(range(epoch_losses[0][\"word_loss\"]),epoch_losses[0][\"word_loss\"],c=\"blue\",label=\"Word Loss\")\n",
    "    ax.plot(range(epoch_losses[0][\"rs_loss\"]),epoch_losses[0][\"rs_loss\"],c=\"red\",label=\"RS Loss\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_train(model,data,opt,loss_func,context_pred_loss_weight=0.5,increments=100):\n",
    "    \"\"\"\n",
    "    * we train with one sample\n",
    "    * recommend from the rest of the candidate pool\n",
    "    * record recommendation\n",
    "    * remove recommendation from candidate pool\n",
    "    * use this recommendation to train again\n",
    "    * repeat for n increments\n",
    "    \"\"\"\n",
    "    # create copies of the data and assign them to candidate pool\n",
    "    \n",
    "    articles = data[\"text\"].tolist()\n",
    "    context_words = data[\"context_word\"].tolist()\n",
    "    class_labels = data[\"class_label\"].tolist()\n",
    "    word_labels = data[\"word_label\"].tolist()\n",
    "        \n",
    "    bert_tokenizer = load_tokenizer()\n",
    "    \n",
    "    bert_tokenized_words, bert_tokenized_word_to_predict, rec_labels, word_labels = tokenize_4bert_batch(articles,\n",
    "                                                                                                         context_words, \n",
    "                                                                                                         class_labels, \n",
    "                                                                                                         word_labels, \n",
    "                                                                                                         tokenizer=bert_tokenizer)\n",
    "    candidate_x1 = bert_tokenized_words\n",
    "    candidate_x2 = bert_tokenized_word_to_predict\n",
    "    candidate_y1 = rec_labels\n",
    "    candidate_y2 = word_labels\n",
    "    \n",
    "    y1_preds = []\n",
    "    all_relevant = sum(class_labels)\n",
    "    \n",
    "    ttl = []\n",
    "    rsl = []\n",
    "    cwl = []\n",
    "    \n",
    "    for i in range(increments):\n",
    "        \n",
    "        predicted_probas_y1 = []\n",
    "        predicted_probas_y2 = []\n",
    "        attention_vectors = []\n",
    "        with torch.no_grad(): # eval or recommendation mode\n",
    "            for i in range(candidate_x1.size(0)):\n",
    "                print(i)\n",
    "                print(candidate_x1.size())\n",
    "                print(candidate_x2.size())\n",
    "                pp_y1, pp_y2, at = model(candidate_x1[i,:],candidate_x2[i,:])\n",
    "                predicted_probas_y1.append(y1)\n",
    "                predicted_probas_y2.append(y2)\n",
    "                attention_vectors.append(at)\n",
    "        \n",
    "        predicted_probas_y1 = torch.cat(predicted_probas_y1,dim=0)\n",
    "        predicted_probas_y2 = torch.cat(predicted_probas_y2,dim=0)\n",
    "        attention_vectors = torch.cat(attention_vectors,dim=0)\n",
    "        \n",
    "        print(str(predicted_probas_y1.size()))\n",
    "        print(str(predicted_probas_y2.size()))\n",
    "        print(str(attention_vectors.size()))\n",
    "        \n",
    "        # get argmax of predicted_probas_y1\n",
    "        rec_item_ind = torch.topk(predicted_probas_y1, k=1, dim=0)[1]\n",
    "        print(str(rec_item_ind.item()))\n",
    "        rec_item_y1_label = candidate_y1[rec_item_ind,:]\n",
    "        print(str(rec_item_y1_label))\n",
    "        \n",
    "        y1_preds.append(rec_item_y1_label)\n",
    "        # delete this index from tensors (not trivial as inn numpy)\n",
    "        # need to subsample the tensor instead of deletion \n",
    "        # tensor = tensor[:,torch.arange(a.size(1))!=rec_item_ind,:]\n",
    "        candidate_x1 = candidate_x1[torch.arange(candidate_x1.size(0)) != rec_item_ind,:]\n",
    "        candidate_x2 = candidate_x2[torch.arange(candidate_x2.size(0)) != rec_item_ind,:]\n",
    "        candidate_y1 = candidate_y1[torch.arange(candidate_y1.size(0)) != rec_item_ind,:]\n",
    "        candidate_y2 = candidate_y2[torch.arange(candidate_y2.size(0)) != rec_item_ind,:]\n",
    "        \n",
    "        \n",
    "        _,_,_,rec_loss, word_loss, total_loss = partial_train_step(x1,x2,model,opt,loss_func,context_pred_loss_weight=0.5)\n",
    "        \n",
    "        ttl.append(total_loss)\n",
    "        rsl.append(rec_loss)\n",
    "        cwl.append(word_loss)\n",
    "    \n",
    "    # calculate metrics \n",
    "    \n",
    "    losses = defaultdict()\n",
    "    losses[\"total_loss\"] = ttl\n",
    "    losses[\"rec_loss\"] = rsl\n",
    "    losses[\"word_loss\"] = cwl\n",
    "    \n",
    "    return losses, y1_preds, all_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding IDF threshold by checking distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running : select_first10\n",
      "Running : to_lower\n",
      "Running : remove_punc\n",
      "Running : remove_small_words\n",
      "Running : remove_spaces\n",
      "Running : remove_non_alpha_numeric\n",
      "\n",
      "Finished running 'preprocess_texts' in 0.0652 mins\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_data(file_path = data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size : 18908\n"
     ]
    }
   ],
   "source": [
    "vocab_idf = get_vocab(df[\"processed_title\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8))\n",
    "sns.histplot(list(vocab_idf.values()),ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Using Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size : 18908\n",
      "Vocab Size After Filtering : 13687\n",
      "Failed to find context words for : 4880 docs\n"
     ]
    }
   ],
   "source": [
    "df[\"context_word_pos\"] = sample_context_words(text_list=df[\"processed_title\"],\n",
    "                                              idf_thresh=9.0,\n",
    "                                              pos_sample_num=3,\n",
    "                                              neg_sample_num=3,\n",
    "                                              cword_type=\"pos\",\n",
    "                                              filter_word_pieces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source_partisan_score</th>\n",
       "      <th>binary_ps</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_title</th>\n",
       "      <th>context_word_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top general warns that 'divisiveness leads to ...</td>\n",
       "      <td>(medianame) America's most senior general warn...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>medianame america most senior general warned ...</td>\n",
       "      <td>top general warns that divisiveness leads to d...</td>\n",
       "      <td>[defeat, pol, grow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How the US government created a fake universit...</td>\n",
       "      <td>The Department of Homeland Security created a ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>the department homeland security created fake ...</td>\n",
       "      <td>how the us government created fake university ...</td>\n",
       "      <td>[university, created, DROP_THIS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's Time To Say It: Trump Is Handling COVID-1...</td>\n",
       "      <td>US President Donald Trump is handling the coro...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>president donald trump handling the coronavir...</td>\n",
       "      <td>its time to say it trump is handling covid19 l...</td>\n",
       "      <td>[handling, dictator, DROP_THIS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump's draconian border lockdown has a new ta...</td>\n",
       "      <td>For the past year, the bridges that cross from...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>for the past year the bridges that cross from ...</td>\n",
       "      <td>trumps draconian border lockdown has new target</td>\n",
       "      <td>[lock, target, DROP_THIS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supreme Court clears way for execution of fede...</td>\n",
       "      <td>TERRE HAUTE, Ind. (AP)  The Trump administrat...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>terre haute ind the trump administration was m...</td>\n",
       "      <td>supreme court clears way for execution of fede...</td>\n",
       "      <td>[clears, execution, prisoner]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>After testy call with Trump over border wall, ...</td>\n",
       "      <td>One Mexican official said Trump lost his temp...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>one mexican official said trump lost his tempe...</td>\n",
       "      <td>after testy call with trump over border wall m...</td>\n",
       "      <td>[mexican, shelves, DROP_THIS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Opinion | If Congress had any pride, it would ...</td>\n",
       "      <td>This pertains to the almost 800,000 dreamers...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>this pertains the almost 800 000 dreamers our ...</td>\n",
       "      <td>opinion if congress had any pride it would set...</td>\n",
       "      <td>[pride, DROP_THIS, DROP_THIS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Liz Cheney on Ukraine: 'Starting to Seem Like ...</td>\n",
       "      <td>Rep. Liz Cheney (R-WY) on Monday said an intel...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>rep liz cheney monday said intelligence offici...</td>\n",
       "      <td>liz cheney on ukraine starting to seem like po...</td>\n",
       "      <td>[starting, seem, liz]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ThinkProgress Smears Dan Crenshaw on Universa...</td>\n",
       "      <td>At ThinkProgress, Josh Israel miscasts Dan Cre...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>thinkprogress josh israel miscasts dan crensh...</td>\n",
       "      <td>thinkprogress smears dan crenshaw on universal...</td>\n",
       "      <td>[checks, universal, background]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pompeo recalls Afghan peace envoy after Trump ...</td>\n",
       "      <td>Secretary of State Mike Pompeo said Sunday tha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>secretary state mike pompeo said sunday that t...</td>\n",
       "      <td>pompeo recalls afghan peace envoy after trump ...</td>\n",
       "      <td>[camp, recalls, afghan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Top general warns that 'divisiveness leads to ...   \n",
       "1  How the US government created a fake universit...   \n",
       "2  It's Time To Say It: Trump Is Handling COVID-1...   \n",
       "3  Trump's draconian border lockdown has a new ta...   \n",
       "4  Supreme Court clears way for execution of fede...   \n",
       "5  After testy call with Trump over border wall, ...   \n",
       "6  Opinion | If Congress had any pride, it would ...   \n",
       "7  Liz Cheney on Ukraine: 'Starting to Seem Like ...   \n",
       "8  ThinkProgress Smears Dan Crenshaw on Universa...   \n",
       "9  Pompeo recalls Afghan peace envoy after Trump ...   \n",
       "\n",
       "                                                text  source_partisan_score  \\\n",
       "0  (medianame) America's most senior general warn...                   -1.0   \n",
       "1  The Department of Homeland Security created a ...                   -1.0   \n",
       "2  US President Donald Trump is handling the coro...                   -2.0   \n",
       "3  For the past year, the bridges that cross from...                   -2.0   \n",
       "4  TERRE HAUTE, Ind. (AP)  The Trump administrat...                    1.0   \n",
       "5  One Mexican official said Trump lost his temp...                   -1.0   \n",
       "6  This pertains to the almost 800,000 dreamers...                   -1.0   \n",
       "7  Rep. Liz Cheney (R-WY) on Monday said an intel...                    2.0   \n",
       "8  At ThinkProgress, Josh Israel miscasts Dan Cre...                    2.0   \n",
       "9  Secretary of State Mike Pompeo said Sunday tha...                    1.0   \n",
       "\n",
       "   binary_ps                                     processed_text  \\\n",
       "0          0   medianame america most senior general warned ...   \n",
       "1          0  the department homeland security created fake ...   \n",
       "2          0   president donald trump handling the coronavir...   \n",
       "3          0  for the past year the bridges that cross from ...   \n",
       "4          1  terre haute ind the trump administration was m...   \n",
       "5          0  one mexican official said trump lost his tempe...   \n",
       "6          0  this pertains the almost 800 000 dreamers our ...   \n",
       "7          1  rep liz cheney monday said intelligence offici...   \n",
       "8          1   thinkprogress josh israel miscasts dan crensh...   \n",
       "9          1  secretary state mike pompeo said sunday that t...   \n",
       "\n",
       "                                     processed_title  \\\n",
       "0  top general warns that divisiveness leads to d...   \n",
       "1  how the us government created fake university ...   \n",
       "2  its time to say it trump is handling covid19 l...   \n",
       "3    trumps draconian border lockdown has new target   \n",
       "4  supreme court clears way for execution of fede...   \n",
       "5  after testy call with trump over border wall m...   \n",
       "6  opinion if congress had any pride it would set...   \n",
       "7  liz cheney on ukraine starting to seem like po...   \n",
       "8  thinkprogress smears dan crenshaw on universal...   \n",
       "9  pompeo recalls afghan peace envoy after trump ...   \n",
       "\n",
       "                   context_word_pos  \n",
       "0               [defeat, pol, grow]  \n",
       "1  [university, created, DROP_THIS]  \n",
       "2   [handling, dictator, DROP_THIS]  \n",
       "3         [lock, target, DROP_THIS]  \n",
       "4     [clears, execution, prisoner]  \n",
       "5     [mexican, shelves, DROP_THIS]  \n",
       "6     [pride, DROP_THIS, DROP_THIS]  \n",
       "7             [starting, seem, liz]  \n",
       "8   [checks, universal, background]  \n",
       "9           [camp, recalls, afghan]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size : 18908\n",
      "Vocab Size After Filtering : 13687\n",
      "Failed to find context words for : 0 docs\n"
     ]
    }
   ],
   "source": [
    "df[\"context_word_neg\"] = sample_context_words(text_list=df[\"processed_title\"],\n",
    "                                              idf_thresh=9.0,\n",
    "                                              neg_sample_num=3,\n",
    "                                              cword_type=\"neg\",\n",
    "                                              filter_word_pieces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source_partisan_score</th>\n",
       "      <th>binary_ps</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_title</th>\n",
       "      <th>context_word_pos</th>\n",
       "      <th>context_word_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top general warns that 'divisiveness leads to ...</td>\n",
       "      <td>(medianame) America's most senior general warn...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>medianame america most senior general warned ...</td>\n",
       "      <td>top general warns that divisiveness leads to d...</td>\n",
       "      <td>[defeat, pol, grow]</td>\n",
       "      <td>[puerto, rely, martini]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How the US government created a fake universit...</td>\n",
       "      <td>The Department of Homeland Security created a ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>the department homeland security created fake ...</td>\n",
       "      <td>how the us government created fake university ...</td>\n",
       "      <td>[university, created, DROP_THIS]</td>\n",
       "      <td>[est, waking, vinci]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's Time To Say It: Trump Is Handling COVID-1...</td>\n",
       "      <td>US President Donald Trump is handling the coro...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>president donald trump handling the coronavir...</td>\n",
       "      <td>its time to say it trump is handling covid19 l...</td>\n",
       "      <td>[handling, dictator, DROP_THIS]</td>\n",
       "      <td>[favored, groom, horse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump's draconian border lockdown has a new ta...</td>\n",
       "      <td>For the past year, the bridges that cross from...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>for the past year the bridges that cross from ...</td>\n",
       "      <td>trumps draconian border lockdown has new target</td>\n",
       "      <td>[lock, target, DROP_THIS]</td>\n",
       "      <td>[gangster, showered, hammered]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supreme Court clears way for execution of fede...</td>\n",
       "      <td>TERRE HAUTE, Ind. (AP)  The Trump administrat...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>terre haute ind the trump administration was m...</td>\n",
       "      <td>supreme court clears way for execution of fede...</td>\n",
       "      <td>[clears, execution, prisoner]</td>\n",
       "      <td>[capacity, financially, expensive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>After testy call with Trump over border wall, ...</td>\n",
       "      <td>One Mexican official said Trump lost his temp...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>one mexican official said trump lost his tempe...</td>\n",
       "      <td>after testy call with trump over border wall m...</td>\n",
       "      <td>[mexican, shelves, DROP_THIS]</td>\n",
       "      <td>[stokes, satirical, swear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Opinion | If Congress had any pride, it would ...</td>\n",
       "      <td>This pertains to the almost 800,000 dreamers...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>this pertains the almost 800 000 dreamers our ...</td>\n",
       "      <td>opinion if congress had any pride it would set...</td>\n",
       "      <td>[pride, DROP_THIS, DROP_THIS]</td>\n",
       "      <td>[replacing, implicit, fiat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Liz Cheney on Ukraine: 'Starting to Seem Like ...</td>\n",
       "      <td>Rep. Liz Cheney (R-WY) on Monday said an intel...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>rep liz cheney monday said intelligence offici...</td>\n",
       "      <td>liz cheney on ukraine starting to seem like po...</td>\n",
       "      <td>[starting, seem, liz]</td>\n",
       "      <td>[joke, trivial, jarrett]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ThinkProgress Smears Dan Crenshaw on Universa...</td>\n",
       "      <td>At ThinkProgress, Josh Israel miscasts Dan Cre...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>thinkprogress josh israel miscasts dan crensh...</td>\n",
       "      <td>thinkprogress smears dan crenshaw on universal...</td>\n",
       "      <td>[checks, universal, background]</td>\n",
       "      <td>[mad, sara, vineyard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pompeo recalls Afghan peace envoy after Trump ...</td>\n",
       "      <td>Secretary of State Mike Pompeo said Sunday tha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>secretary state mike pompeo said sunday that t...</td>\n",
       "      <td>pompeo recalls afghan peace envoy after trump ...</td>\n",
       "      <td>[camp, recalls, afghan]</td>\n",
       "      <td>[janice, identity, franchise]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Top general warns that 'divisiveness leads to ...   \n",
       "1  How the US government created a fake universit...   \n",
       "2  It's Time To Say It: Trump Is Handling COVID-1...   \n",
       "3  Trump's draconian border lockdown has a new ta...   \n",
       "4  Supreme Court clears way for execution of fede...   \n",
       "5  After testy call with Trump over border wall, ...   \n",
       "6  Opinion | If Congress had any pride, it would ...   \n",
       "7  Liz Cheney on Ukraine: 'Starting to Seem Like ...   \n",
       "8  ThinkProgress Smears Dan Crenshaw on Universa...   \n",
       "9  Pompeo recalls Afghan peace envoy after Trump ...   \n",
       "\n",
       "                                                text  source_partisan_score  \\\n",
       "0  (medianame) America's most senior general warn...                   -1.0   \n",
       "1  The Department of Homeland Security created a ...                   -1.0   \n",
       "2  US President Donald Trump is handling the coro...                   -2.0   \n",
       "3  For the past year, the bridges that cross from...                   -2.0   \n",
       "4  TERRE HAUTE, Ind. (AP)  The Trump administrat...                    1.0   \n",
       "5  One Mexican official said Trump lost his temp...                   -1.0   \n",
       "6  This pertains to the almost 800,000 dreamers...                   -1.0   \n",
       "7  Rep. Liz Cheney (R-WY) on Monday said an intel...                    2.0   \n",
       "8  At ThinkProgress, Josh Israel miscasts Dan Cre...                    2.0   \n",
       "9  Secretary of State Mike Pompeo said Sunday tha...                    1.0   \n",
       "\n",
       "   binary_ps                                     processed_text  \\\n",
       "0          0   medianame america most senior general warned ...   \n",
       "1          0  the department homeland security created fake ...   \n",
       "2          0   president donald trump handling the coronavir...   \n",
       "3          0  for the past year the bridges that cross from ...   \n",
       "4          1  terre haute ind the trump administration was m...   \n",
       "5          0  one mexican official said trump lost his tempe...   \n",
       "6          0  this pertains the almost 800 000 dreamers our ...   \n",
       "7          1  rep liz cheney monday said intelligence offici...   \n",
       "8          1   thinkprogress josh israel miscasts dan crensh...   \n",
       "9          1  secretary state mike pompeo said sunday that t...   \n",
       "\n",
       "                                     processed_title  \\\n",
       "0  top general warns that divisiveness leads to d...   \n",
       "1  how the us government created fake university ...   \n",
       "2  its time to say it trump is handling covid19 l...   \n",
       "3    trumps draconian border lockdown has new target   \n",
       "4  supreme court clears way for execution of fede...   \n",
       "5  after testy call with trump over border wall m...   \n",
       "6  opinion if congress had any pride it would set...   \n",
       "7  liz cheney on ukraine starting to seem like po...   \n",
       "8  thinkprogress smears dan crenshaw on universal...   \n",
       "9  pompeo recalls afghan peace envoy after trump ...   \n",
       "\n",
       "                   context_word_pos                    context_word_neg  \n",
       "0               [defeat, pol, grow]             [puerto, rely, martini]  \n",
       "1  [university, created, DROP_THIS]                [est, waking, vinci]  \n",
       "2   [handling, dictator, DROP_THIS]             [favored, groom, horse]  \n",
       "3         [lock, target, DROP_THIS]      [gangster, showered, hammered]  \n",
       "4     [clears, execution, prisoner]  [capacity, financially, expensive]  \n",
       "5     [mexican, shelves, DROP_THIS]          [stokes, satirical, swear]  \n",
       "6     [pride, DROP_THIS, DROP_THIS]         [replacing, implicit, fiat]  \n",
       "7             [starting, seem, liz]            [joke, trivial, jarrett]  \n",
       "8   [checks, universal, background]               [mad, sara, vineyard]  \n",
       "9           [camp, recalls, afghan]       [janice, identity, franchise]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Ten Files : ['/media/karthikshivaram/SABER_4TB/bert_embeddings/0.npy', '/media/karthikshivaram/SABER_4TB/bert_embeddings/1.npy', '/media/karthikshivaram/SABER_4TB/bert_embeddings/2.npy', '/media/karthikshivaram/SABER_4TB/bert_embeddings/3.npy', '/media/karthikshivaram/SABER_4TB/bert_embeddings/4.npy', '/media/karthikshivaram/SABER_4TB/bert_embeddings/5.npy', '/media/karthikshivaram/SABER_4TB/bert_embeddings/6.npy', '/media/karthikshivaram/SABER_4TB/bert_embeddings/7.npy', '/media/karthikshivaram/SABER_4TB/bert_embeddings/8.npy', '/media/karthikshivaram/SABER_4TB/bert_embeddings/9.npy']\n",
      "Processing Batch No : 200\n",
      "Processing Batch No : 300\n",
      "Processing Batch No : 400\n",
      "Processing Batch No : 500\n",
      "Processing Batch No : 600\n",
      "Processing Batch No : 700\n",
      "Processing Batch No : 800\n",
      "Processing Batch No : 900\n",
      "Processing Batch No : 1000\n",
      "Processing Batch No : 1100\n",
      "Processing Batch No : 1200\n",
      "Processing Batch No : 1300\n",
      "Processing Batch No : 1400\n",
      "Processing Batch No : 1500\n",
      "Processing Batch No : 1600\n",
      "Processing Batch No : 1700\n",
      "Processing Batch No : 1800\n",
      "Processing Batch No : 1900\n",
      "\n",
      "Finished running 'load_bert_output' in 69.2760 mins\n",
      "\n",
      "\n",
      "Finished running 'load_bert_embeddings' in 69.2760 mins\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors = load_bert_embeddings(df=df,\n",
    "                                saved_path=\"/media/karthikshivaram/SABER_4TB/bert_embeddings\",\n",
    "                                batch_size=50,\n",
    "                                layer=12,\n",
    "                                context_var=100,\n",
    "                                aggregation=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices to drop : 4880\n",
      "                                               title  \\\n",
      "0  Top general warns that 'divisiveness leads to ...   \n",
      "1  How the US government created a fake universit...   \n",
      "\n",
      "                                                text  source_partisan_score  \\\n",
      "0  (medianame) America's most senior general warn...                   -1.0   \n",
      "1  The Department of Homeland Security created a ...                   -1.0   \n",
      "\n",
      "   binary_ps                                     processed_text  \\\n",
      "0          0   medianame america most senior general warned ...   \n",
      "1          0  the department homeland security created fake ...   \n",
      "\n",
      "                                     processed_title  \\\n",
      "0  top general warns that divisiveness leads to d...   \n",
      "1  how the us government created fake university ...   \n",
      "\n",
      "                   context_word_pos         context_word_neg  drop_  \n",
      "0               [defeat, pol, grow]  [puerto, rely, martini]  False  \n",
      "1  [university, created, DROP_THIS]     [est, waking, vinci]  False  \n",
      "New Df Shape : (95120, 9)\n",
      "\n",
      "Running KMEANS Clustering with k=100\n",
      "\n",
      "Finished running 'run_clustering' in 0.0397 mins\n",
      "\n",
      "\n",
      "Finished running 'cluster2doc' in 0.0003 mins\n",
      "\n",
      "\n",
      "Finished running 'get_cluster_sizes' in 0.0002 mins\n",
      "\n",
      "\n",
      "Number of Cluster Pairs : 4950\n",
      "\n",
      "Finished running 'get_cluster_pairs' in 0.0000 mins\n",
      "\n",
      "\n",
      "Finished running 'filter_clusters' in 0.0185 mins\n",
      "\n",
      "\n",
      "Number of Filtered Cluster Pairs are greater 100, picking top 100 most similar cluster pairs\n",
      "\n",
      "Finished running 'get_pairwise_dist' in 0.0001 mins\n",
      "\n",
      "\n",
      "Finished running 'get_top_100_clusterpairs' in 0.0000 mins\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_df , doc_2_cluster_map, top100 = get_cluster_pairs_top(df,vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# clean_df.to_csv(\"temp_df.csv\",index=False)\n",
    "# with open('d2c_map.pickle', 'wb') as handle:\n",
    "#     pickle.dump(doc_2_cluster_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('top100.pickle', 'wb') as handle:\n",
    "#     pickle.dump(top100, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import ast\n",
    "# clean_df = pd.read_csv(\"temp_df.csv\")\n",
    "# clean_df[\"context_word_neg\"] = clean_df[\"context_word_neg\"].apply(lambda x : ast.literal_eval(x))\n",
    "# doc_2_cluster_map = None\n",
    "# top100 = None\n",
    "# with open('d2c_map.pickle', 'rb') as handle:\n",
    "#     doc_2_cluster_map = pickle.load(handle)\n",
    "# with open('top100.pickle', 'rb') as handle:\n",
    "#     top100 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none_df = clean_df.loc[clean_df[\"context_word_pos\"] == \"DROP_THIS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_df = clean_df[clean_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source_partisan_score</th>\n",
       "      <th>binary_ps</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_title</th>\n",
       "      <th>context_word_pos</th>\n",
       "      <th>context_word_neg</th>\n",
       "      <th>drop_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top general warns that 'divisiveness leads to ...</td>\n",
       "      <td>(medianame) America's most senior general warn...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>medianame america most senior general warned ...</td>\n",
       "      <td>top general warns that divisiveness leads to d...</td>\n",
       "      <td>[defeat, pol, grow]</td>\n",
       "      <td>[puerto, rely, martini]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How the US government created a fake universit...</td>\n",
       "      <td>The Department of Homeland Security created a ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>the department homeland security created fake ...</td>\n",
       "      <td>how the us government created fake university ...</td>\n",
       "      <td>[university, created, DROP_THIS]</td>\n",
       "      <td>[est, waking, vinci]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's Time To Say It: Trump Is Handling COVID-1...</td>\n",
       "      <td>US President Donald Trump is handling the coro...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>president donald trump handling the coronavir...</td>\n",
       "      <td>its time to say it trump is handling covid19 l...</td>\n",
       "      <td>[handling, dictator, DROP_THIS]</td>\n",
       "      <td>[favored, groom, horse]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump's draconian border lockdown has a new ta...</td>\n",
       "      <td>For the past year, the bridges that cross from...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>for the past year the bridges that cross from ...</td>\n",
       "      <td>trumps draconian border lockdown has new target</td>\n",
       "      <td>[lock, target, DROP_THIS]</td>\n",
       "      <td>[gangster, showered, hammered]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supreme Court clears way for execution of fede...</td>\n",
       "      <td>TERRE HAUTE, Ind. (AP)  The Trump administrat...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>terre haute ind the trump administration was m...</td>\n",
       "      <td>supreme court clears way for execution of fede...</td>\n",
       "      <td>[clears, execution, prisoner]</td>\n",
       "      <td>[capacity, financially, expensive]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>After testy call with Trump over border wall, ...</td>\n",
       "      <td>One Mexican official said Trump lost his temp...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>one mexican official said trump lost his tempe...</td>\n",
       "      <td>after testy call with trump over border wall m...</td>\n",
       "      <td>[mexican, shelves, DROP_THIS]</td>\n",
       "      <td>[stokes, satirical, swear]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Opinion | If Congress had any pride, it would ...</td>\n",
       "      <td>This pertains to the almost 800,000 dreamers...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>this pertains the almost 800 000 dreamers our ...</td>\n",
       "      <td>opinion if congress had any pride it would set...</td>\n",
       "      <td>[pride, DROP_THIS, DROP_THIS]</td>\n",
       "      <td>[replacing, implicit, fiat]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Liz Cheney on Ukraine: 'Starting to Seem Like ...</td>\n",
       "      <td>Rep. Liz Cheney (R-WY) on Monday said an intel...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>rep liz cheney monday said intelligence offici...</td>\n",
       "      <td>liz cheney on ukraine starting to seem like po...</td>\n",
       "      <td>[starting, seem, liz]</td>\n",
       "      <td>[joke, trivial, jarrett]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ThinkProgress Smears Dan Crenshaw on Universa...</td>\n",
       "      <td>At ThinkProgress, Josh Israel miscasts Dan Cre...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>thinkprogress josh israel miscasts dan crensh...</td>\n",
       "      <td>thinkprogress smears dan crenshaw on universal...</td>\n",
       "      <td>[checks, universal, background]</td>\n",
       "      <td>[mad, sara, vineyard]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pompeo recalls Afghan peace envoy after Trump ...</td>\n",
       "      <td>Secretary of State Mike Pompeo said Sunday tha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>secretary state mike pompeo said sunday that t...</td>\n",
       "      <td>pompeo recalls afghan peace envoy after trump ...</td>\n",
       "      <td>[camp, recalls, afghan]</td>\n",
       "      <td>[janice, identity, franchise]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Top general warns that 'divisiveness leads to ...   \n",
       "1  How the US government created a fake universit...   \n",
       "2  It's Time To Say It: Trump Is Handling COVID-1...   \n",
       "3  Trump's draconian border lockdown has a new ta...   \n",
       "4  Supreme Court clears way for execution of fede...   \n",
       "5  After testy call with Trump over border wall, ...   \n",
       "6  Opinion | If Congress had any pride, it would ...   \n",
       "7  Liz Cheney on Ukraine: 'Starting to Seem Like ...   \n",
       "8  ThinkProgress Smears Dan Crenshaw on Universa...   \n",
       "9  Pompeo recalls Afghan peace envoy after Trump ...   \n",
       "\n",
       "                                                text  source_partisan_score  \\\n",
       "0  (medianame) America's most senior general warn...                   -1.0   \n",
       "1  The Department of Homeland Security created a ...                   -1.0   \n",
       "2  US President Donald Trump is handling the coro...                   -2.0   \n",
       "3  For the past year, the bridges that cross from...                   -2.0   \n",
       "4  TERRE HAUTE, Ind. (AP)  The Trump administrat...                    1.0   \n",
       "5  One Mexican official said Trump lost his temp...                   -1.0   \n",
       "6  This pertains to the almost 800,000 dreamers...                   -1.0   \n",
       "7  Rep. Liz Cheney (R-WY) on Monday said an intel...                    2.0   \n",
       "8  At ThinkProgress, Josh Israel miscasts Dan Cre...                    2.0   \n",
       "9  Secretary of State Mike Pompeo said Sunday tha...                    1.0   \n",
       "\n",
       "   binary_ps                                     processed_text  \\\n",
       "0          0   medianame america most senior general warned ...   \n",
       "1          0  the department homeland security created fake ...   \n",
       "2          0   president donald trump handling the coronavir...   \n",
       "3          0  for the past year the bridges that cross from ...   \n",
       "4          1  terre haute ind the trump administration was m...   \n",
       "5          0  one mexican official said trump lost his tempe...   \n",
       "6          0  this pertains the almost 800 000 dreamers our ...   \n",
       "7          1  rep liz cheney monday said intelligence offici...   \n",
       "8          1   thinkprogress josh israel miscasts dan crensh...   \n",
       "9          1  secretary state mike pompeo said sunday that t...   \n",
       "\n",
       "                                     processed_title  \\\n",
       "0  top general warns that divisiveness leads to d...   \n",
       "1  how the us government created fake university ...   \n",
       "2  its time to say it trump is handling covid19 l...   \n",
       "3    trumps draconian border lockdown has new target   \n",
       "4  supreme court clears way for execution of fede...   \n",
       "5  after testy call with trump over border wall m...   \n",
       "6  opinion if congress had any pride it would set...   \n",
       "7  liz cheney on ukraine starting to seem like po...   \n",
       "8  thinkprogress smears dan crenshaw on universal...   \n",
       "9  pompeo recalls afghan peace envoy after trump ...   \n",
       "\n",
       "                   context_word_pos                    context_word_neg  drop_  \n",
       "0               [defeat, pol, grow]             [puerto, rely, martini]  False  \n",
       "1  [university, created, DROP_THIS]                [est, waking, vinci]  False  \n",
       "2   [handling, dictator, DROP_THIS]             [favored, groom, horse]  False  \n",
       "3         [lock, target, DROP_THIS]      [gangster, showered, hammered]  False  \n",
       "4     [clears, execution, prisoner]  [capacity, financially, expensive]  False  \n",
       "5     [mexican, shelves, DROP_THIS]          [stokes, satirical, swear]  False  \n",
       "6     [pride, DROP_THIS, DROP_THIS]         [replacing, implicit, fiat]  False  \n",
       "7             [starting, seem, liz]            [joke, trivial, jarrett]  False  \n",
       "8   [checks, universal, background]               [mad, sara, vineyard]  False  \n",
       "9           [camp, recalls, afghan]       [janice, identity, franchise]  False  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(doc_2_cluster_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_pair : (20, 99)\n",
      "Train Shape : (865, 9)\n",
      "Test Shape : (958, 9)\n",
      "Original class_labels : \n",
      "1815    0\n",
      "2463    1\n",
      "2932    0\n",
      "1953    1\n",
      "4578    0\n",
      "3882    1\n",
      "725     1\n",
      "225     0\n",
      "4182    1\n",
      "4605    1\n",
      "Name: class_label, dtype: int64\n",
      "Flipped class_labels : \n",
      "1815    1\n",
      "2463    0\n",
      "2932    1\n",
      "1953    0\n",
      "4578    1\n",
      "3882    0\n",
      "725     0\n",
      "225     1\n",
      "4182    0\n",
      "4605    0\n",
      "Name: class_label, dtype: int64\n",
      "Train Shape : (4781, 4)\n",
      "Test Shape : (5127, 4)\n",
      "Train Shape : (4781, 4)\n",
      "Test Shape : (5127, 4)\n",
      "                                                text context_word  word_label  \\\n",
      "0  somebody is stocking up on the vietnam wars mo...        stock           1   \n",
      "1  bloomberg out the gate  were happening russia ...         gate           1   \n",
      "2  hey big spender michael bloomberg spending loa...          hey           1   \n",
      "3  rnc spox 50000 new donors since democrat impea...        hours           1   \n",
      "4  affirmative action restoration plan to appear ...  affirmative           1   \n",
      "5  gregg jarrett an fbi that is corrupt and disho...        proof           1   \n",
      "6   facts on gun crime that show gun control does...        lucas           0   \n",
      "7  va hospital hides jesus behind curtain  may ha...        means           0   \n",
      "8  seventeen democratic hopefuls blitz iowa steak...    seventeen           1   \n",
      "9  patel policy innovator donald trump not many p...        patel           1   \n",
      "\n",
      "   class_label  \n",
      "0            0  \n",
      "1            1  \n",
      "2            1  \n",
      "3            1  \n",
      "4            1  \n",
      "5            1  \n",
      "6            1  \n",
      "7            1  \n",
      "8            0  \n",
      "9            1  \n",
      "Epoch : 0 | Batch : 100 | Total Loss : 51.595611572265625 | Rec Loss : 1.5956121683120728 | Word Loss : 100.0\n",
      "Epoch : 0 | Batch : 200 | Total Loss : 1.234614372253418 | Rec Loss : 0.6762282252311707 | Word Loss : 1.1167724132537842\n",
      "Epoch : 0 | Batch : 300 | Total Loss : 1.0150423049926758 | Rec Loss : 0.6639840602874756 | Word Loss : 0.7021164298057556\n",
      "Epoch : 0 | Batch : 400 | Total Loss : 1.4131662845611572 | Rec Loss : 0.912684977054596 | Word Loss : 1.000962495803833\n",
      "Epoch : 0 | Batch : 500 | Total Loss : 1.0102107524871826 | Rec Loss : 0.6113865375518799 | Word Loss : 0.7976483106613159\n",
      "Epoch : 0 | Batch : 600 | Total Loss : 1.2398356199264526 | Rec Loss : 0.7421140670776367 | Word Loss : 0.9954431056976318\n",
      "Epoch : 0 | Batch : 700 | Total Loss : 0.967303454875946 | Rec Loss : 0.7792764902114868 | Word Loss : 0.37605389952659607\n",
      "Epoch : 0 | Batch : 800 | Total Loss : 1.409374475479126 | Rec Loss : 0.9929519891738892 | Word Loss : 0.8328449130058289\n",
      "Epoch : 0 | Batch : 900 | Total Loss : 0.9052498936653137 | Rec Loss : 0.6438528895378113 | Word Loss : 0.5227940082550049\n",
      "Epoch : 0 | Batch : 1000 | Total Loss : 1.1595362424850464 | Rec Loss : 0.7980461120605469 | Word Loss : 0.722980260848999\n",
      "Epoch : 0 | Batch : 1100 | Total Loss : 0.9284886121749878 | Rec Loss : 0.6159263253211975 | Word Loss : 0.6251245737075806\n",
      "Epoch : 0 | Batch : 1200 | Total Loss : 1.1172170639038086 | Rec Loss : 0.7573482990264893 | Word Loss : 0.7197375893592834\n",
      "Epoch : 0 | Batch : 1300 | Total Loss : 1.1541898250579834 | Rec Loss : 0.7325746417045593 | Word Loss : 0.8432302474975586\n",
      "Epoch : 0 | Batch : 1400 | Total Loss : 1.2063231468200684 | Rec Loss : 0.8413105010986328 | Word Loss : 0.7300254106521606\n",
      "Epoch : 0 | Batch : 1500 | Total Loss : 0.5946422815322876 | Rec Loss : 0.4688110947608948 | Word Loss : 0.25166231393814087\n",
      "Epoch : 0 | Batch : 1600 | Total Loss : 1.2381749153137207 | Rec Loss : 0.8783040046691895 | Word Loss : 0.7197417616844177\n",
      "Epoch : 0 | Batch : 1700 | Total Loss : 0.9650930762290955 | Rec Loss : 0.6159346699714661 | Word Loss : 0.6983168125152588\n",
      "Epoch : 0 | Batch : 1800 | Total Loss : 1.1620042324066162 | Rec Loss : 0.7211317420005798 | Word Loss : 0.8817449808120728\n",
      "Epoch : 0 | Batch : 1900 | Total Loss : 1.092928409576416 | Rec Loss : 0.7698126435279846 | Word Loss : 0.6462315320968628\n",
      "Epoch : 0 | Batch : 2000 | Total Loss : 0.8798500299453735 | Rec Loss : 0.5832111835479736 | Word Loss : 0.5932777523994446\n",
      "Epoch : 0 | Batch : 2100 | Total Loss : 0.884230375289917 | Rec Loss : 0.631091296672821 | Word Loss : 0.5062781572341919\n",
      "Epoch : 0 | Batch : 2200 | Total Loss : 0.9101858139038086 | Rec Loss : 0.5768861770629883 | Word Loss : 0.6665993332862854\n",
      "Epoch : 0 | Batch : 2300 | Total Loss : 1.249729871749878 | Rec Loss : 0.9560703039169312 | Word Loss : 0.5873192548751831\n",
      "Epoch : 0 | Batch : 2400 | Total Loss : 0.9331377744674683 | Rec Loss : 0.5834113359451294 | Word Loss : 0.699452817440033\n",
      "Epoch : 0 | Batch : 2500 | Total Loss : 1.0292725563049316 | Rec Loss : 0.6357348561286926 | Word Loss : 0.7870754599571228\n",
      "Epoch : 0 | Batch : 2600 | Total Loss : 1.1328649520874023 | Rec Loss : 0.7086153626441956 | Word Loss : 0.8484991788864136\n",
      "Epoch : 0 | Batch : 2700 | Total Loss : 1.2956812381744385 | Rec Loss : 0.8691884875297546 | Word Loss : 0.8529853820800781\n",
      "Epoch : 0 | Batch : 2800 | Total Loss : 1.1548020839691162 | Rec Loss : 0.7834557890892029 | Word Loss : 0.7426925897598267\n",
      "Epoch : 0 | Batch : 2900 | Total Loss : 1.0386205911636353 | Rec Loss : 0.7553693056106567 | Word Loss : 0.566502571105957\n",
      "Epoch : 0 | Batch : 3000 | Total Loss : 0.9983429908752441 | Rec Loss : 0.606944739818573 | Word Loss : 0.7827965617179871\n",
      "Epoch : 0 | Batch : 3100 | Total Loss : 1.145594835281372 | Rec Loss : 0.7825499176979065 | Word Loss : 0.7260898351669312\n",
      "Epoch : 0 | Batch : 3200 | Total Loss : 1.369762659072876 | Rec Loss : 0.9120873808860779 | Word Loss : 0.9153506755828857\n",
      "Epoch : 0 | Batch : 3300 | Total Loss : 1.055418848991394 | Rec Loss : 0.6738824248313904 | Word Loss : 0.7630727887153625\n",
      "Epoch : 0 | Batch : 3400 | Total Loss : 0.9860349893569946 | Rec Loss : 0.6307891607284546 | Word Loss : 0.7104915976524353\n",
      "Epoch : 0 | Batch : 3500 | Total Loss : 1.1466102600097656 | Rec Loss : 0.795420229434967 | Word Loss : 0.7023800015449524\n",
      "Epoch : 0 | Batch : 3600 | Total Loss : 1.1608184576034546 | Rec Loss : 0.8049368858337402 | Word Loss : 0.7117630839347839\n",
      "Epoch : 0 | Batch : 3700 | Total Loss : 1.10086989402771 | Rec Loss : 0.7728458642959595 | Word Loss : 0.6560479402542114\n",
      "Epoch : 0 | Batch : 3800 | Total Loss : 1.1753675937652588 | Rec Loss : 0.8929859399795532 | Word Loss : 0.5647634267807007\n",
      "Epoch : 0 | Batch : 3900 | Total Loss : 1.1388803720474243 | Rec Loss : 0.8273680210113525 | Word Loss : 0.6230247616767883\n",
      "Epoch : 0 | Batch : 4000 | Total Loss : 1.0071916580200195 | Rec Loss : 0.7220361232757568 | Word Loss : 0.5703111290931702\n",
      "Epoch : 0 | Batch : 4100 | Total Loss : 1.0056989192962646 | Rec Loss : 0.6132313013076782 | Word Loss : 0.7849351763725281\n",
      "Epoch : 0 | Batch : 4200 | Total Loss : 1.166733741760254 | Rec Loss : 0.8161497712135315 | Word Loss : 0.7011678218841553\n",
      "Epoch : 0 | Batch : 4300 | Total Loss : 0.9658669233322144 | Rec Loss : 0.6214799880981445 | Word Loss : 0.6887739300727844\n",
      "Epoch : 0 | Batch : 4400 | Total Loss : 1.3401734828948975 | Rec Loss : 0.8472034931182861 | Word Loss : 0.9859399795532227\n",
      "Epoch : 0 | Batch : 4500 | Total Loss : 0.9714365005493164 | Rec Loss : 0.49878746271133423 | Word Loss : 0.9452980160713196\n",
      "Epoch : 0 | Batch : 4600 | Total Loss : 0.7457437515258789 | Rec Loss : 0.5218100547790527 | Word Loss : 0.44786742329597473\n",
      "Epoch : 0 | Batch : 4700 | Total Loss : 0.9314563274383545 | Rec Loss : 0.6199948191642761 | Word Loss : 0.6229230165481567\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a7f3245ba558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model, epoch_losses, opt, loss_func = run_all_cps(clean_df,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                   \u001b[0;34m[\u001b[0m\u001b[0mtop100\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                   \u001b[0mdoc_2_cluster_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                   \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                   \u001b[0mcontext_pred_loss_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "model, epoch_losses, opt, loss_func = run_all_cps(clean_df,\n",
    "                                                  [top100[8]],\n",
    "                                                  doc_2_cluster_map,\n",
    "                                                  learning_rate=0.01,\n",
    "                                                  context_pred_loss_weight=0.5,\n",
    "                                                  num_epochs=1,\n",
    "                                                  batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Frames",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
