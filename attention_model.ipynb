{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....... Initializing Settings ..... \n",
      "Random_Seed Chosen : 15112\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from bert_utils import load_tokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from preprocess_utils import preprocess_texts\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import itertools\n",
    "from bert_embeddings import load_bert_embeddings\n",
    "from clustering_utils import run_clustering, get_cluster_sizes, score_cluster, get_cluster_pairs, get_pairwise_dist, cluster2doc, filter_clusters, get_top_100_clusterpairs\n",
    "from general_utils import timer\n",
    "import config as CONFIG\n",
    "import  gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(CONFIG.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcc7c0e0c50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(CONFIG.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams['axes.grid'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "-------\n",
    "* Reasons a document may not get a context word when sampling\n",
    "    * IDF thresh\n",
    "    * Unkown (missing from vocabulary)\n",
    "    * when tokenized all are word pieces (if not observing word pieces)\n",
    "    * Contains only stopwords and numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still Working On :\n",
    "-------------------\n",
    "* Flip labels for heterogeneous - **done**\n",
    "* Replace tokens for pos samples - **done**\n",
    "* Batch size > 2 - **done**\n",
    "* Iterative training - **done**\n",
    "* batch prediciton \n",
    "* cross val for parameter choosing\n",
    "* split train for val set \n",
    "* freeze bert layer 12 option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow :\n",
    "------\n",
    "\n",
    "* Dataset Prep\n",
    "    * Preprocess\n",
    "    * context word generation\n",
    "* Bert Embeddings For Clustering\n",
    "* Clustering\n",
    "* Picking Cluster Pairs\n",
    "* For each Cluster Pair :\n",
    "    * Training\n",
    "    * Metric Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../sampled_articles_from_relevant_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_b = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in test_b.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_utils\n",
    "\n",
    "# Need:\n",
    "# 1) A way to generate vocabulary to sample from\n",
    "# 2) Sampling Context Words\n",
    "# 3) Negative Binary Sampling to generate negative labels for the context words\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    tokenizer = load_tokenizer()\n",
    "    df = pd.read_csv(data_path)\n",
    "    df.drop(columns=[\"all_text\"],inplace=True)\n",
    "    df[\"processed_title\"] = preprocess_texts(df[\"title\"].tolist())\n",
    "    return df\n",
    "    \n",
    "\n",
    "def calc_idf(vocab_dict,num_docs = 100000):\n",
    "    \"\"\"\n",
    "    idf_t = log(N/df_t)\n",
    "    \n",
    "    Large it is rarer the term, smaller it is more frequent the term\n",
    "    \"\"\"\n",
    "    return {key: np.log2(num_docs/vocab_dict[key]) for key in vocab_dict.keys()}\n",
    "\n",
    "def filter_vocab(vocab_idf_dict,thresh=10.0,filter_word_pieces=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # remove words from title vocab that have very low idf values and are stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    vocab_idf = dict(filter(lambda x: x[1]>=thresh, vocab_idf_dict.items()))\n",
    "    vocab_idf = dict(filter(lambda x: x[0] not in stop_words, vocab_idf.items()))\n",
    "    vocab_idf = dict(filter(lambda x: \"UNK\" not in x[0], vocab_idf.items())) # Token absent from bert's vocab\n",
    "    vocab_idf = dict(filter(lambda x: not x[0].isnumeric(), vocab_idf.items())) # Don't include numbers to represent context words\n",
    "    vocab_idf = dict(filter(lambda x: len(x[0]) >= 3,vocab_idf.items())) # filter out small words\n",
    "    \n",
    "    if filter_word_pieces:\n",
    "        vocab_idf = dict(filter(lambda x: \"##\" not in x[0], vocab_idf.items()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return vocab_idf\n",
    "\n",
    "def get_vocab(text_list):\n",
    "    \"\"\"\n",
    "    Here we get the vocab to sample from for obtaining our context word also has idf information\n",
    "    \n",
    "    Strategy 1: Using words from the title\n",
    "    \"\"\"\n",
    "    tokens_list = []\n",
    "    tokenizer = load_tokenizer()\n",
    "    vocab_df = defaultdict(int)\n",
    "    for text in text_list:\n",
    "        token_ids = tokenizer.encode(text,add_special_tokens=False)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        \n",
    "        for token in tokens:\n",
    "            vocab_df[token] +=1\n",
    "        \n",
    "    vocab_idf = calc_idf(vocab_df)\n",
    "    \n",
    "    print(\"Vocab_size : %s\" %str(len(vocab_idf.keys())))\n",
    "    \n",
    "    return vocab_idf\n",
    "\n",
    "def replace_context_word(token_text,context_word):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    token_text.remove(context_word)\n",
    "    \n",
    "    return token_text\n",
    "\n",
    "def sample_context_words(text_list,idf_thresh=10.0,pos_sample_num=3,neg_sample_num=3,cword_type=\"pos\",filter_word_pieces=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    vocab_idf = get_vocab(text_list)\n",
    "    vocab_idf = filter_vocab(vocab_idf,\n",
    "                             thresh=idf_thresh,\n",
    "                             filter_word_pieces=filter_word_pieces)\n",
    "    \n",
    "    print(\"Vocab Size After Filtering : %s\" %str(len(vocab_idf.keys())))\n",
    "    tokenizer = load_tokenizer()\n",
    "    \n",
    "    if cword_type == \"pos\":\n",
    "        \n",
    "        context_words = []\n",
    "        fails = 0\n",
    "        for text in text_list :\n",
    "            context_word = sample_context_words_pos(text,vocab_idf,tokenizer,filter_word_pieces=filter_word_pieces,pos_sample_num=pos_sample_num)\n",
    "            if \"DROP_THIS\" in Counter(context_word) and Counter(context_word)[\"DROP_THIS\"] == pos_sample_num:\n",
    "                fails+=1\n",
    "            context_words.append(context_word)\n",
    "        \n",
    "        print(\"Failed to find context words for : %s docs\" %str(fails))\n",
    "        return context_words\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        context_words = []\n",
    "        fails= 0\n",
    "        for text in text_list :\n",
    "            # result is list of lists where list[0] has length = neg_sample_num\n",
    "            con_wl = sample_context_words_neg(text,vocab_idf,tokenizer,neg_sample_num=neg_sample_num)\n",
    "            if con_wl == None:\n",
    "                fails+=1\n",
    "            context_words.append(con_wl)\n",
    "            \n",
    "        print(\"Failed to find context words for : %s docs\" %str(fails))\n",
    "        return context_words\n",
    "\n",
    "def sample_context_words_pos(text,vocab_idf,tokenizer,pos_sample_num=3,filter_word_pieces=True):\n",
    "    \"\"\"\n",
    "    Sample from titles vocab but don't include stopwords or low-idf terms (frequent terms)\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text,add_special_tokens=False))\n",
    "    \n",
    "    if filter_word_pieces:\n",
    "        tokens = [t for t in tokens if \"#\" not in t]\n",
    "    \n",
    "    sampled_indices = random.sample(range(len(tokens)),k=len(tokens))\n",
    "    context_word = None\n",
    "    loop_counter = 0\n",
    "    temp_c = 0\n",
    "    context_words = []\n",
    "    for indice in sampled_indices:\n",
    "        if temp_c == pos_sample_num:\n",
    "            break\n",
    "        context_word = tokens[sampled_indices[indice]]\n",
    "        if context_word in vocab_idf:\n",
    "            context_words.append(context_word)\n",
    "            context_word = None\n",
    "            temp_c +=1\n",
    "            \n",
    "    # Nones to add:\n",
    "    nones_to_add = pos_sample_num - len(context_words)\n",
    "    if nones_to_add > 0:\n",
    "        context_words += [\"DROP_THIS\"] * nones_to_add\n",
    "    \n",
    "    return context_words\n",
    "\n",
    "\n",
    "def sample_context_words_neg(text,vocab_idf,tokenizer,neg_sample_num=3):\n",
    "    \"\"\"\n",
    "    Similar to Pos context word sampling \n",
    "    \n",
    "    sample from titles vocab but don't include stopwords or low-idf terms (frequent terms) and not in text\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(text,add_special_tokens=False))\n",
    "    \n",
    "    context_words = None\n",
    "    hard_stop = 0\n",
    "    while True:\n",
    "        context_words = random.sample(list(vocab_idf.keys()),k=neg_sample_num)\n",
    "        hard_stop += 1\n",
    "        not_in_vocab = 0\n",
    "        for c_w in context_words:\n",
    "            if c_w not in tokens:\n",
    "                not_in_vocab +=1\n",
    "        \n",
    "        if not_in_vocab == neg_sample_num:\n",
    "            break\n",
    "        \n",
    "        if hard_stop == 11:\n",
    "            context_words = None\n",
    "            break\n",
    "    \n",
    "    return context_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_text_gen(input_1_list,\n",
    "                   input_2_list,\n",
    "                   output_1_list,\n",
    "                   output_2_list,\n",
    "                   which_cluster,batch_size=500):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for ndx in range(0,len(input_1_list),batch_size):\n",
    "        yield input_1_list[ndx:min(ndx+batch_size,len(input_1_list))], input_2_list[ndx:min(ndx+batch_size,len(input_2_list))], output_1_list[ndx:min(ndx+batch_size,len(output_1_list))], output_2_list[ndx:min(ndx+batch_size,len(output_2_list))], which_cluster[ndx:min(ndx+batch_size,len(which_cluster))]\n",
    "\n",
    "def batch_gen_pred(x1_tensor,x2_tensor,y1_tensor,y2_tensor,batch_size):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    for ndx in range(0,list(x1_tensor.size())[0],batch_size):\n",
    "        yield x1_tensor[ndx:min(ndx+batch_size,list(x1_tensor.size())[0]),:],  x2_tensor[ndx:min(ndx+batch_size,list(x2_tensor.size())[0]),:], y1_tensor[ndx:min(ndx+batch_size,list(y1_tensor.size())[0])], y2_tensor[ndx:min(ndx+batch_size,list(y2_tensor.size())[0])]\n",
    "\n",
    "def tokenize_4bert_batch(input1,input2,output1,output2,tokenizer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cuda1 = torch.device('cuda:1')\n",
    "    \n",
    "    input_1_cor = []\n",
    "    \n",
    "    for a,b in zip(input1,input2):\n",
    "        input_1_cor.append(a.replace(b,\"\"))\n",
    "    \n",
    "    \n",
    "    tokenized_tensor = torch.LongTensor([tokenizer.encode(text,\n",
    "                                                          truncation=True,\n",
    "                                                          padding=\"max_length\",\n",
    "                                                          max_length=500, \n",
    "                                                          add_special_tokens=True)\n",
    "                                                          for text in input_1_cor])\n",
    "    \n",
    "    tokenized_context_word = torch.LongTensor([tokenizer.encode(word,truncation=True,add_special_tokens=False,max_length=1) for word in input2])\n",
    "    \n",
    "    class_labels = torch.FloatTensor(output1)\n",
    "    word_labels = torch.FloatTensor(output2)\n",
    "    \n",
    "    tokenized_tensor = tokenized_tensor.to(cuda1)\n",
    "    tokenized_context_word = tokenized_context_word.to(cuda1)\n",
    "    class_labels = class_labels.to(cuda1)\n",
    "    word_labels = word_labels.to(cuda1)\n",
    "    \n",
    "    return tokenized_tensor, tokenized_context_word, class_labels, word_labels\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "class AttentionMT(nn.Module):\n",
    "    \n",
    "    def __init__(self,embedding_size=768,verbose=True,bert_train=False):\n",
    "        super(AttentionMT,self).__init__()\n",
    "        \n",
    "        cuda1 = torch.device('cuda:1')\n",
    "        self.verbose = verbose\n",
    "        self.bert_train = bert_train\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "        self.bert.to(cuda1)\n",
    "        if self.bert_train:\n",
    "            # Set the pre-trained model in training mode (for Fine Tuning)\n",
    "            self.bert.train()\n",
    "        if not self.bert_train:\n",
    "            # Set the pre-trained model in evaluation mode (for inference only)\n",
    "            self.bert.eval()\n",
    "        \n",
    "        self.attention = nn.Linear(in_features=embedding_size,\n",
    "                                   out_features=1,\n",
    "                                   bias=False)\n",
    "        \n",
    "        # Recommendation Network\n",
    "        # ------------------------\n",
    "        \n",
    "        self.recom_pred = nn.Linear(in_features=embedding_size,\n",
    "                                    out_features=1,\n",
    "                                    bias=True)\n",
    "        \n",
    "        # Softmax Activation\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # Sigmoid Activation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,bert_tokenized_words,bert_tokenized_word_to_predict):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "#         return self.forward_single(bert_tokenized_words,bert_tokenized_word_to_predict)\n",
    "        return self.forward_batch(bert_tokenized_words,bert_tokenized_word_to_predict)\n",
    "        \n",
    "    def forward_single(self,\n",
    "                bert_tokenized_words,\n",
    "                bert_tokenized_word_to_predict):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        ------\n",
    "        \n",
    "        Outputs:\n",
    "        --------\n",
    "        \"\"\"\n",
    "        \n",
    "        # ************ Recommendation Network ********************\n",
    "        \n",
    "        # output shape should be (max_length,dim)\n",
    "        bert_output = self.bert(input_ids=bert_tokenized_words)\n",
    "        bert_hidden_states  = bert_output[2]\n",
    "        bert_layer12_hidden_states = bert_hidden_states[-1][0,:,:]\n",
    "\n",
    "        attention_un = torch.cat([self.attention(embedding) for embedding in bert_layer12_hidden_states])  # shape (N,max_length,1) *** u_it **\n",
    "        attentions = self.softmax(attention_un) # shape (N,max_length,1) ***  a_it ***\n",
    "        attention_cvector = bert_layer12_hidden_states.T.mul(attentions).sum(dim=1) # shape (N,768,1) *** s_i ***\n",
    "        y_pred = self.sigmoid(self.recom_pred(attention_cvector.T))\n",
    "        \n",
    "        # ************** Word Prediction Network ********************\n",
    "        context_word_embed_bert = self.bert(input_ids=bert_tokenized_word_to_predict)\n",
    "        context_word_embed_bert_hs = context_word_embed_bert[2]\n",
    "        context_word_embed_bert_layer_12 = context_word_embed_bert_hs[-1][0,:,:]\n",
    "        \n",
    "        context_pred = self.sigmoid(torch.mul(attention_cvector,context_word_embed_bert_layer_12).sum(dim=1))\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\nShape Details :\")\n",
    "            print(\"1. Bert Embeddings Shape : %s\" %str(bert_layer12_hidden_states.size()))\n",
    "            print(\"2. attention_un Shape : %s\" %str(attention_un.size()))\n",
    "            print(\"3. attention_norm Shape : %s\" %str(attentions.size()))\n",
    "            print(\"4. attention_cvector shape : %s\" %str(attention_cvector.size()))\n",
    "            print(\"5. y_pred shape : %s\" %str(y_pred.size()))\n",
    "            print(str(y_pred.item()))\n",
    "            print(\"6. context_word_embed_bert shape : %s\" %str(context_word_embed_bert_layer_12.size()))\n",
    "            print(\"7. context_pred shape : %s\" %str(context_pred.size()))\n",
    "            print(str(context_pred.item()))\n",
    "        \n",
    "        return y_pred, context_pred, attention_cvector\n",
    "    \n",
    "    def forward_batch(self,\n",
    "                      bert_tokenized_words_batch,\n",
    "                      bert_tokenized_word_to_predict_batch):\n",
    "        \"\"\"\n",
    "        Shape Details :\n",
    "        \n",
    "        when considering single samples\n",
    "        1. Bert Embeddings Shape : torch.Size([500, 768])\n",
    "        2. attention_un Shape : torch.Size([500])\n",
    "        3. attention_norm Shape : torch.Size([500])\n",
    "        4. attention_cvector shape : torch.Size([768])\n",
    "        5. y_pred shape : torch.Size([1])\n",
    "        0.5326072573661804\n",
    "        6. context_word_embed_bert shape : torch.Size([1, 768])\n",
    "        7. context_pred shape : torch.Size([1])\n",
    "        \n",
    "        when considering batches\n",
    "        1. Bert Embeddings Shape : (N,500,768)-> Done\n",
    "        2. attention_un Shape : (N,500)\n",
    "        3. attention_norm Shape : (N,500)\n",
    "        4. attention_cvector Shape : (N,768)\n",
    "        5. ypred Shape : (N,1)\n",
    "        6. context_word_embed_bert shape : torch.Size([N, 768])\n",
    "        7. context_pred shape : torch.Size([N,1])\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # ************* REC NETWORK *********************\n",
    "        \n",
    "        bert_output = self.bert(input_ids=bert_tokenized_words_batch)\n",
    "        bert_hidden_states  = bert_output[2]\n",
    "        bert_layer12_hidden_states = bert_hidden_states[-1]\n",
    "        \n",
    "        # Unormalized Word Attentions\n",
    "        atten_un_batch = []\n",
    "        for item_in_batch in range(bert_layer12_hidden_states.size(0)):\n",
    "            atten_un_sent = []\n",
    "            for word_in_sent in range(bert_layer12_hidden_states.size(1)):\n",
    "                word_atten_un = self.attention(bert_layer12_hidden_states[item_in_batch,word_in_sent,:]).unsqueeze(0) # should be of size [1,1]\n",
    "                atten_un_sent.append(word_atten_un)  \n",
    "            \n",
    "            atten_un_sent = torch.cat(atten_un_sent,dim=0).T # should have shape (1,500)\n",
    "            atten_un_batch.append(atten_un_sent)\n",
    "        \n",
    "        atten_un_batch = torch.cat(atten_un_batch,dim=0) # should have shape (N,500)\n",
    "        \n",
    "        \n",
    "#         atten_un_batch = self.attention(bert_layer12_hidden_states) # should have shape (N,500,1)\n",
    "#         print(\"Unormalized Attention Shape : %s\" %str(atten_un_batch.size()))\n",
    "        \n",
    "        \n",
    "        # Normalized Word Attentions\n",
    "        attentions_norm_batch = self.softmax(atten_un_batch).squeeze(-1) # should have shape (N,500) or (N,500,1)\n",
    "#         print(\"Normalized Attention Shape : %s\"%str(attentions_norm_batch.size()))\n",
    "        \n",
    "        # Sentence Context vector\n",
    "        attention_cvector_batch = []\n",
    "        \n",
    "        for item_in_batch in range(bert_layer12_hidden_states.size(0)):\n",
    "            attention_cvector = bert_layer12_hidden_states[item_in_batch,:,:].T.mul(attentions_norm_batch[item_in_batch,:]).sum(dim=1) # shape (1,768)\n",
    "            attention_cvector_batch.append(attention_cvector.unsqueeze(0)) \n",
    "        \n",
    "        \n",
    "        attention_cvector_batch = torch.cat(attention_cvector_batch,dim=0) # shape (N,768) *** s_i ***\n",
    "#         print(\"Sentence context vector shape : %s\" %str(attention_cvector_batch.size()))\n",
    "        \n",
    "        # Output Layer 1\n",
    "        y_pred = self.sigmoid(self.recom_pred(attention_cvector_batch)).squeeze() # shape (N,1)\n",
    "#         print(\"Output Layer 1 Shape : %s\" %str(y_pred.shape))\n",
    "        # **************** WORD PRED NETWORK ****************   \n",
    "        context_word_embed_bert = self.bert(input_ids=bert_tokenized_word_to_predict_batch)\n",
    "        context_word_embed_bert_hs = context_word_embed_bert[2]\n",
    "        context_word_embed_bert_layer_12 = context_word_embed_bert_hs[-1] # shape (N,1,768)\n",
    "        \n",
    "        attention_wcprod_batch = []\n",
    "        for item_in_batch in range(context_word_embed_bert_layer_12.size(0)):\n",
    "            \n",
    "            attention_wc_prod = torch.mul(attention_cvector_batch[item_in_batch,:],\n",
    "                                          context_word_embed_bert_layer_12[item_in_batch,:,:]).sum(dim=1) # shape (1,1)\n",
    "            attention_wcprod_batch.append(attention_wc_prod)\n",
    "        \n",
    "        attention_wcprod_batch = torch.cat(attention_wcprod_batch,dim=0) # shape (N,1)\n",
    "#         print(\"Attention Word Context Product : %s\" %str(attention_wcprod_batch.size()))\n",
    "        context_pred = self.sigmoid(attention_wcprod_batch) # shape (N,1)\n",
    "#         print(\"Output Layer 2 Shape : %s\" %str(context_pred.size()))\n",
    "    \n",
    "        return y_pred, context_pred, attention_cvector_batch\n",
    "\n",
    "def reset_weights(m):\n",
    "    \"\"\"\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "    \"\"\"\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(\"Reset trainable parameters of layer : %s\"%str(layer))\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering utils\n",
    "\n",
    "def get_cluster_pairs_top(df,vectors,sample_size=3):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Now drop the rows with Nones for context_word_pos and similarly do the same to the embeddings\n",
    "    \n",
    "    df[\"drop_\"] = df['context_word_pos'].apply(lambda x: x == [\"DROP_THIS\"]*sample_size)\n",
    "    \n",
    "    indices_to_drop = df.index[df['drop_'] == True].tolist()\n",
    "    print(\"Indices to drop : %s\" %str(len(indices_to_drop)))\n",
    "    clean_df = df.drop(df.index[indices_to_drop])\n",
    "    clean_df.reset_index(drop=True,inplace=True)\n",
    "    print(clean_df.head(2))\n",
    "    print(\"New Df Shape : %s\" %str(clean_df.shape))\n",
    "    \n",
    "    vectors = np.delete(vectors,indices_to_drop,axis=0)\n",
    "    \n",
    "    clusters,cluster_clf = run_clustering(vectors=vectors,\n",
    "                                              seed=CONFIG.RANDOM_SEED,\n",
    "                                              num_clusters=CONFIG.num_clusters,\n",
    "                                              clus_type=\"kmeans\")\n",
    "    \n",
    "    doc_2_cluster_map = cluster2doc(num_texts= clean_df.shape[0],\n",
    "                                    cluster_labels=cluster_clf.labels_)\n",
    "    \n",
    "    \n",
    "    cluster_sizes = get_cluster_sizes(cluster_clf)\n",
    "    \n",
    "    cluster_pairs = get_cluster_pairs(num_clusters=CONFIG.num_clusters)\n",
    "    \n",
    "    filtered_cluster_pairs = filter_clusters(cluster_pairs=cluster_pairs,\n",
    "                                            doc_2_cluster_map=doc_2_cluster_map,\n",
    "                                            cluster_sizes=cluster_sizes,\n",
    "                                            partisan_scores=clean_df[\"binary_ps\"].tolist(),\n",
    "                                            min_size=CONFIG.min_cluster_size,\n",
    "                                            max_size=CONFIG.max_cluster_size,\n",
    "                                            min_partisan_size=CONFIG.min_partisan_size)\n",
    "    \n",
    "    top100 = None\n",
    "    \n",
    "    if len(filtered_cluster_pairs) > 100:\n",
    "        print(\"\\nNumber of Filtered Cluster Pairs are greater 100, picking top 100 most similar cluster pairs\")\n",
    "        cluster_pair_dist_mat = get_pairwise_dist(cluster_clf,dist_type=\"cosine\")\n",
    "        top100 = get_top_100_clusterpairs(cluster_pairs=filtered_cluster_pairs,dist_matrix=cluster_pair_dist_mat,reverse=True)\n",
    "        \n",
    "    else:\n",
    "        top100 = filtered_cluster_pairs\n",
    "        print(\"\\nNumber of Filtered Cluster Pairs is less than 100 so skipping top 100 selection\")\n",
    "    \n",
    "    return clean_df , doc_2_cluster_map, top100 \n",
    "\n",
    "def get_label_dist(df,columns=[\"class_label\",\"word_label\"]):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for c in columns:\n",
    "        print(\"%s dist :\\n\"%c)\n",
    "        print(df[c].value_counts())\n",
    "\n",
    "def get_train_test_ssda(df,cp,doc_2_cluster_map,neg_sample_size=3):\n",
    "    \"\"\"\n",
    "    * train = 90% c1 data + 10% c2 data\n",
    "    * test = 50% c1 data + 50% c2 data\n",
    "    \n",
    "    * From cluster 1 choose 70% of the data for train, 30% for test\n",
    "    * From cluster 2 choose 10% of 70% from c1 for train, test of cluster 1 test size\n",
    "    \"\"\"\n",
    "    c1_df, c2_df = get_train_test_attm(df,cp,doc_2_cluster_map,neg_sample_size=neg_sample_size)\n",
    "    c1_df[\"which_cluster\"] = [1]*c1_df.shape[0]\n",
    "    c2_df[\"which_cluster\"] = [2]*c2_df.shape[0]\n",
    "    \n",
    "    c1_train, c1_test = train_test_split(c1_df,test_size=0.30, random_state=CONFIG.RANDOM_SEED)\n",
    "    \n",
    "    c1_train_num = c1_train.shape[0]\n",
    "    \n",
    "    c2_df_train,c2_df_test = train_test_split(c2_df,train_size=int(0.1*c1_train_num), random_state=CONFIG.RANDOM_SEED)\n",
    "    \n",
    "    print(\"\\nSample size from C1 in Train : %s\" %str(c1_train.shape))\n",
    "    get_label_dist(c1_train)\n",
    "    print(\"\\nSample size from C2 in Train : %s\" %str(c2_df_train.shape))\n",
    "    get_label_dist(c2_df_train)\n",
    "    \n",
    "    train = pd.concat([c1_train,c2_df_train],axis=0)\n",
    "    print(\"\\nTrain Size : %s\"%str(train.shape))\n",
    "    get_label_dist(train)\n",
    "    \n",
    "    c2_test_,_ = train_test_split(c2_df_test,train_size=c1_test.shape[0], random_state=CONFIG.RANDOM_SEED)\n",
    "    \n",
    "    print(\"\\nSample Size from C1 in Test : %s\" %str(c1_test.shape))\n",
    "    get_label_dist(c1_test)\n",
    "    print(\"\\nSample Size from C2 in Test : %s\" %str(c2_test_.shape))\n",
    "    get_label_dist(c2_test_)\n",
    "    \n",
    "    test = pd.concat([c1_test,c2_test_],axis=0)\n",
    "    print(\"\\nTest Size : %s\" %str(test.shape))\n",
    "    get_label_dist(test)\n",
    "    \n",
    "    train = train.sample(frac=1.0,random_state=CONFIG.RANDOM_SEED)\n",
    "    test = test.sample(frac=1.0,random_state=CONFIG.RANDOM_SEED)\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "\n",
    "\n",
    "def mask_arrays(yp_1,yp_2,y1,y2,which_cluster,cluster_2_mask=1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    yp_1_ma = []\n",
    "    yp_2_ma = []\n",
    "    y1_ma = []\n",
    "    y2_ma = []\n",
    "    \n",
    "    for wc_i , cluster in enumerate(which_cluster):\n",
    "        if cluster != cluster_2_mask:\n",
    "            yp_1_ma.append(yp_1[wc_i])\n",
    "            yp_2_ma.append(yp_2[wc_i])\n",
    "            y1_ma.append(y1[wc_i])\n",
    "            y2_ma.append(y2[wc_i])\n",
    "    \n",
    "    return yp_1_ma, yp_2_ma, y1_ma, y2_ma\n",
    "            \n",
    "\n",
    "def calc_score_(yp_1,yp_2,y1,y2,scores_,key=\"overall\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    yp_1 = np.array(yp_1,d)\n",
    "    yp_2 = np.array(yp_2)\n",
    "    y1 = np.array(y1)\n",
    "    y2 = np.array(y2)\n",
    "     \n",
    "    \n",
    "    yp_1[yp_1 > 0.5] = 1.0\n",
    "    yp_1[yp_1 <= 0.5] = 0.0\n",
    "    yp_2[yp_2 > 0.5] = 1.0\n",
    "    yp_2[yp_2 <=0.5] = 0.0\n",
    "    \n",
    "    if 0.0 not in yp_1 or 1.0 not in yp_1:\n",
    "        print(\"One class predicitions for class labels\")\n",
    "    \n",
    "    if 0.0 not in yp_2 or 1.0 not in yp_2: \n",
    "        print(\"One class predictions for word labels\")\n",
    "    \n",
    "    f1 = metrics.f1_score(y1,yp_1,average=\"macro\")\n",
    "    prec = metrics.precision_score(y1,yp_1,average=\"macro\")\n",
    "    recall = metrics.recall_score(y1,yp_1,average=\"macro\")\n",
    "    try:\n",
    "        roc_auc = metrics.roc_auc_score(y1,yp_1,average=\"macro\")\n",
    "    except:\n",
    "        print(\"ROC Error\")\n",
    "        roc_auc = 0.0\n",
    "    accuracy = metrics.accuracy_score(y1,yp_1)\n",
    "    \n",
    "    scores_[key][\"class_scores\"][\"f1\"] = f1\n",
    "    scores_[key][\"class_scores\"][\"precision\"] = prec\n",
    "    scores_[key][\"class_scores\"][\"recall\"] = recall\n",
    "    scores_[key][\"class_scores\"][\"accuracy\"] = roc_auc\n",
    "    scores_[key][\"class_scores\"][\"roc_auc\"] = accuracy\n",
    "    \n",
    "    f1 = metrics.f1_score(y2,yp_2,average=\"macro\")\n",
    "    prec = metrics.precision_score(y2,yp_2,average=\"macro\")\n",
    "    recall = metrics.recall_score(y2,yp_2,average=\"macro\")\n",
    "    try:\n",
    "        roc_auc = metrics.roc_auc_score(y2,yp_2,average=\"macro\")\n",
    "    except:\n",
    "        print(\"ROC Error\")\n",
    "        roc_auc = 0.0\n",
    "    accuracy = metrics.accuracy_score(y2,yp_2)\n",
    "    \n",
    "    scores_[key][\"word_scores\"][\"f1\"] = f1\n",
    "    scores_[key][\"word_scores\"][\"precision\"] = prec\n",
    "    scores_[key][\"word_scores\"][\"recall\"] = recall\n",
    "    scores_[key][\"word_scores\"][\"accuracy\"] = roc_auc\n",
    "    scores_[key][\"word_scores\"][\"roc_auc\"] = accuracy\n",
    "    \n",
    "    return scores_\n",
    "    \n",
    "\n",
    "def calculate_scores(preds_1,preds_2,true_1,true_2,which_cluster):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    scores_ = defaultdict(lambda : defaultdict(lambda : defaultdict(float)))\n",
    "    # overall_metrics\n",
    "    scores_ = calc_score_(preds_1,preds_2,true_1,true_2,scores_,key=\"overall\")\n",
    "    \n",
    "    # cluster 1 metrics\n",
    "    yp_1_ma, yp_2_ma, y1_ma, y2_ma =  mask_arrays(preds_1,preds_2,true_1,true_2,which_cluster,cluster_2_mask=2)\n",
    "    scores_ = calc_score_(yp_1_ma,yp_2_ma,y1_ma,y2_ma,scores_,key=\"cluster1\")\n",
    "    \n",
    "    # cluster 2 metrics\n",
    "    yp_1_ma, yp_2_ma, y1_ma, y2_ma =  mask_arrays(preds_1,preds_2,true_1,true_2,which_cluster,cluster_2_mask=1)\n",
    "    scores_ = calc_score_(yp_1_ma,yp_2_ma,y1_ma,y2_ma,scores_,key=\"cluster2\")\n",
    "    \n",
    "    return scores_\n",
    "\n",
    "@timer\n",
    "def batch_predict_and_score(model,data,batch_size = 25):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    articles = data[\"text\"].tolist()\n",
    "    context_words = data[\"context_word\"].tolist()\n",
    "    class_labels = data[\"class_label\"].tolist()\n",
    "    word_labels = data[\"word_label\"].tolist()\n",
    "    which_cluster = data[\"which_cluster\"].tolist()\n",
    "    bert_tokenizer = load_tokenizer()\n",
    "    batch_rec_preds = []\n",
    "    batch_context_preds = []\n",
    "    for batch_num, (article_batch, context_word_batch, class_label_batch, word_label_batch, which_cluster_batch) in enumerate(batch_text_gen(articles,\n",
    "                                                                                                                                                 context_words,\n",
    "                                                                                                                                                 class_labels,\n",
    "                                                                                                                                                 word_labels,\n",
    "                                                                                                                                                 which_cluster,\n",
    "                                                                                                                                                 batch_size=batch_size)):\n",
    "            \n",
    "            bert_tokenized_words, bert_tokenized_word_to_predict, rec_labels, word_labels_ = tokenize_4bert_batch(article_batch,\n",
    "                                                                                                                 context_word_batch, \n",
    "                                                                                                                 class_label_batch, \n",
    "                                                                                                                 word_label_batch, tokenizer=bert_tokenizer)\n",
    "            with torch.no_grad():\n",
    "                y_pred, context_pred, attention_vector = model(bert_tokenized_words, bert_tokenized_word_to_predict)\n",
    "                \n",
    "                batch_rec_preds.append(y_pred.cpu().numpy())\n",
    "                batch_context_preds.append(context_pred.cpu().numpy())\n",
    "            \n",
    "            bert_tokenized_words = None\n",
    "            bert_tokenized_word_to_predict = None\n",
    "            rec_labels = None\n",
    "            word_labels_ = None\n",
    "            \n",
    "            del bert_tokenized_words\n",
    "            del bert_tokenized_word_to_predict\n",
    "            del rec_labels\n",
    "            del word_labels_\n",
    "            \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    batch_rec_preds = np.concatenate(batch_rec_preds,axis=0)\n",
    "    batch_context_preds = np.concatenate(batch_context_preds,axis=0)\n",
    "    \n",
    "    scores_ = calculate_scores(batch_rec_preds,batch_context_preds,class_labels,word_labels,which_cluster)\n",
    "    \n",
    "    return scores_\n",
    "                \n",
    "            \n",
    "\n",
    "@timer\n",
    "def train_ssda(data,lr,word_loss_w,epochs=2,batch_size=8):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cuda1 = torch.device('cuda:1')\n",
    "    model = AttentionMT(embedding_size=768,verbose=True)\n",
    "    model.to(cuda1)\n",
    "    loss_func = nn.BCELoss()\n",
    "    opt = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    bert_tokenizer = load_tokenizer()\n",
    "    \n",
    "    epoch_losses = defaultdict(lambda : defaultdict(list))\n",
    "    \n",
    "    articles = data[\"text\"].tolist()\n",
    "    context_words = data[\"context_word\"].tolist()\n",
    "    class_labels = data[\"class_label\"].tolist()\n",
    "    word_labels = data[\"word_label\"].tolist()\n",
    "    which_cluster = data[\"which_cluster\"].tolist()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        batch_nums = data.shape[0]/batch_size\n",
    "        batch_total_loss = []\n",
    "        batch_word_loss = []\n",
    "        batch_rs_loss = []\n",
    "        \n",
    "        for batch_num, (article_batch, context_word_batch, class_label_batch, word_label_batch, which_cluster_batch) in enumerate(batch_text_gen(articles,\n",
    "                                                                                                                                                 context_words,\n",
    "                                                                                                                                                 class_labels,\n",
    "                                                                                                                                                 word_labels,\n",
    "                                                                                                                                                 which_cluster,\n",
    "                                                                                                                                                 batch_size=batch_size)):\n",
    "            \n",
    "            bert_tokenized_words, bert_tokenized_word_to_predict, rec_labels, word_labels_ = tokenize_4bert_batch(article_batch,\n",
    "                                                                                                                 context_word_batch, \n",
    "                                                                                                                 class_label_batch, \n",
    "                                                                                                                 word_label_batch, tokenizer=bert_tokenizer)\n",
    "            opt.zero_grad() # reset all the gradient information\n",
    "    \n",
    "            y_pred, context_pred, attention_vector = model(bert_tokenized_words, bert_tokenized_word_to_predict)\n",
    "            \n",
    "            \n",
    "            if y_pred.size() != rec_labels.size():\n",
    "                rec_labels = rec_labels.squeeze(0)\n",
    "            \n",
    "            if context_pred.size() != word_labels_.size():\n",
    "                context_pred = context_pred.squeeze(0)\n",
    "            \n",
    "            rec_loss = loss_func(y_pred,rec_labels)\n",
    "            word_loss = loss_func(context_pred,word_labels_)\n",
    "            \n",
    "            total_loss = rec_loss + (word_loss_w * word_loss)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            \n",
    "            batch_rs_loss.append(rec_loss.item())\n",
    "            batch_word_loss.append(word_loss.item())\n",
    "            batch_total_loss.append(total_loss.item())\n",
    "            \n",
    "            if batch_num % 100 == 0 and batch_num >=100:\n",
    "                print(\"Epoch : %s | Batch : %s | Total Loss : %s | Rec Loss : %s | Word Loss : %s\" % (str(epoch),str(batch_num),str(total_loss.item()),str(rec_loss.item()),str(word_loss.item())))\n",
    "            \n",
    "            \n",
    "        epoch_losses[epoch][\"rs_loss\"].append(batch_rs_loss)\n",
    "        epoch_losses[epoch][\"word_loss\"].append(batch_word_loss)\n",
    "        epoch_losses[epoch][\"total_loss\"].append(batch_total_loss)\n",
    "        \n",
    "        scores_train =  batch_predict_and_score(model,data,batch_size = 25)\n",
    "    \n",
    "    bert_tokenized_words = None\n",
    "    bert_tokenized_word_to_predict = None\n",
    "    rec_labels = None\n",
    "    word_labels_ = None\n",
    "    rec_loss = None\n",
    "    total_loss = None\n",
    "    word_loss = None\n",
    "    opt = None\n",
    "    \n",
    "\n",
    "    del bert_tokenized_words\n",
    "    del bert_tokenized_word_to_predict\n",
    "    del rec_labels\n",
    "    del word_labels_\n",
    "    del opt\n",
    "    del total_loss\n",
    "    del rec_loss\n",
    "    del word_loss\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return model, epoch_losses, scores_train\n",
    "    \n",
    "    \n",
    "\n",
    "def test_ssda(model,data,batch_size=25):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    scores_ = batch_predict_and_score(model,data,batch_size = batch_size)\n",
    "    return scores_\n",
    "\n",
    "\n",
    "def run_ssda(df,cp,doc_2_cluster_map,learning_rates=[0.0001,0.001,0.01,0.1],epochs=3,word_pred_loss_weights=[0.3,0.8],batch_size=8,neg_sample_size=3):\n",
    "    \"\"\"\n",
    "    Uses a self supervised domain adaptation setting to train the model\n",
    "    \n",
    "    * train = 90% c1 data + 10% c2 data\n",
    "    * test = 50% c1 data + 50% c2 data\n",
    "    \n",
    "    * From cluster 1 choose 70% of the data for train, 30% for test\n",
    "    * From cluster 2 choose 10% of 70% from c1 for train, \n",
    "    \n",
    "    * Loss check\n",
    "    * Metrics = F1, recall, precision, accuracy, roc\n",
    "    \"\"\"\n",
    "    train, test = get_train_test_ssda(df,cp,doc_2_cluster_map,neg_sample_size=neg_sample_size)\n",
    "    metrics_train = defaultdict()\n",
    "    metrics_test = defaultdict()\n",
    "    losses_train = defaultdict()\n",
    "    # train ssda func\n",
    "    for lr in learning_rates:\n",
    "        for word_loss_w in word_pred_loss_weights:\n",
    "            \n",
    "            # FREE MEMORY\n",
    "            model = None\n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            print(\"sleeping for 10\")\n",
    "            time.sleep(10)\n",
    "            \n",
    "            model, losses_train , scores_train = train_ssda(train,lr,word_loss_w,epochs=epochs,batch_size=batch_size)\n",
    "            scores_test = test_ssda(model,test)\n",
    "            metrics_train[(lr,word_loss_w)] = scores_train\n",
    "            metrics_test[(lr,word_loss_w)] = scores_test\n",
    "            losses_train[(lr,word_loss_w)] = losses_train\n",
    "            \n",
    "    return metrics_train,metrics_test, losses_train\n",
    "\n",
    "def run_ssda_all(df,cps,doc_2_cluster_map,learning_rates=[0.0001,0.001,0.01,0.1],epochs=3,word_pred_loss_weights=[0.3,0.8],batch_size=8,neg_sample_size=3):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cp_test_scores = defaultdict()\n",
    "    cp_train_scores = defaultdict()\n",
    "    cp_train_losses = defaultdict()\n",
    "    \n",
    "    for cp in cps:\n",
    "        metrics_train,metrics_test, losses_train = run_ssda(df,\n",
    "                                                            cp,\n",
    "                                                            doc_2_cluster_map,\n",
    "                                                            learning_rates=learning_rates,\n",
    "                                                            epochs=epochs,\n",
    "                                                            word_pred_loss_weights=word_pred_loss_weights,\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            neg_sample_size=neg_sample_size)\n",
    "        \n",
    "        cp_test_scores[cp] = metrics_test\n",
    "        cp_train_scores[cp] = metrics_train\n",
    "        cp_train_losses[cp] = losses_train\n",
    "    \n",
    "    return cp_test_scores, cp_train_scores, cp_train_losses\n",
    "    \n",
    "\n",
    "def gen_samples(df,neg_sample_size=3):\n",
    "    \"\"\"\n",
    "    columns = processed_text,processed_title, context_word_pos, context_word_neg\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"processed_all\"] = df[\"processed_title\"] + \" \" + df[\"processed_text\"]\n",
    "    \n",
    "    text_list = df[\"processed_all\"].tolist()\n",
    "    ps_labels = df[\"binary_ps\"].tolist()\n",
    "    pos_con_word = df[\"context_word_pos\"].tolist()\n",
    "    neg_con_word = df[\"context_word_neg\"].tolist()\n",
    "    \n",
    "    text_list_neg = []\n",
    "    ps_labels_neg = []\n",
    "    text_list_pos = []\n",
    "    ps_labels_pos = []\n",
    "    \n",
    "    for ind_t, text in enumerate(text_list):\n",
    "        text_list_neg.append([text]*neg_sample_size)\n",
    "        ps_labels_neg.append([ps_labels[ind_t]]*neg_sample_size)\n",
    "        text_list_pos.append([text]*neg_sample_size)\n",
    "        ps_labels_pos.append([ps_labels[ind_t]]*neg_sample_size)\n",
    "        \n",
    "    text_list_neg = list(itertools.chain(*text_list_neg))\n",
    "    neg_con_word = list(itertools.chain(*neg_con_word))\n",
    "    ps_labels_neg = list(itertools.chain(*ps_labels_neg))\n",
    "    \n",
    "    text_list_pos = list(itertools.chain(*text_list_pos))\n",
    "    pos_con_word = list(itertools.chain(*pos_con_word))\n",
    "    ps_labels_pos = list(itertools.chain(*ps_labels_pos))\n",
    "    \n",
    "    assert len(text_list_neg) == len(neg_con_word)\n",
    "    assert len(text_list_neg) == len(text_list_pos)\n",
    "    assert len(text_list_pos) == len(pos_con_word)\n",
    "    \n",
    "    all_text_list = text_list_pos + text_list_neg\n",
    "    all_con_word = pos_con_word + neg_con_word\n",
    "    all_word_labels = ([1]*len(pos_con_word)) + ([0] * len(neg_con_word))\n",
    "    all_ps_labels = ps_labels_pos + ps_labels_neg\n",
    "    \n",
    "    df_sample = pd.DataFrame()\n",
    "    df_sample[\"text\"] = all_text_list\n",
    "    df_sample[\"context_word\"] = all_con_word\n",
    "    df_sample[\"word_label\"] = all_word_labels\n",
    "    df_sample[\"class_label\"] = all_ps_labels\n",
    "    \n",
    "    df_sample = df_sample.loc[df_sample[\"context_word\"] != \"DROP_THIS\"].reset_index(drop=True)\n",
    "    \n",
    "    # Shuffle twice\n",
    "    df_sample = df_sample.sample(frac=1.0,random_state=CONFIG.RANDOM_SEED)\n",
    "    df_sample = df_sample.sample(frac=1.0,random_state=CONFIG.RANDOM_SEED+1)\n",
    "    \n",
    "    return df_sample\n",
    "    \n",
    "    \n",
    "\n",
    "def get_train_test_attm(df,cp,doc_2_cluster_map,neg_sample_size=3):\n",
    "    \"\"\"\n",
    "    train and test we need to first subsample using doc_2_cluster_map for each cluster\n",
    "    get train and test , then create negative samples and finally shuffle both train and test\n",
    "    \"\"\"\n",
    "    cluster1_indices = doc_2_cluster_map[cp[0]]\n",
    "    cluster2_indices = doc_2_cluster_map[cp[1]]\n",
    "    \n",
    "    train_df = df.iloc[cluster1_indices].reset_index(drop=True)\n",
    "    test_df = df.iloc[cluster2_indices].reset_index(drop=True)\n",
    "    \n",
    "    print(\"Original Train Shape : %s\" %str(train_df.shape))\n",
    "    print(\"Original Test Shape : %s\" %str(test_df.shape))\n",
    "    \n",
    "    train_df = gen_samples(train_df,neg_sample_size=neg_sample_size)\n",
    "    test_df = gen_samples(test_df,neg_sample_size=neg_sample_size)\n",
    "    \n",
    "#     print(\"Original class_labels : \\n%s\" %str(test_df[\"class_label\"].iloc[:10]))\n",
    "    \n",
    "    test_df[\"class_label\"] = test_df[\"class_label\"].apply(lambda x: np.abs(x+(-1)))\n",
    "    \n",
    "#     print(\"Flipped class_labels : \\n%s\" %str(test_df[\"class_label\"].iloc[:10]))\n",
    "    \n",
    "    train_df = train_df.sample(frac=1,random_state=CONFIG.RANDOM_SEED).reset_index(drop=True)\n",
    "    test_df = test_df.sample(frac=1,random_state=CONFIG.RANDOM_SEED).reset_index(drop=True)\n",
    "    \n",
    "    print(\"Exploded Train Shape : %s\" %str(train_df.shape))\n",
    "    print(\"Exploded Test Shape : %s\" %str(test_df.shape))\n",
    "    return train_df,test_df\n",
    "    \n",
    "\n",
    "def partial_train_step(x1,x2,rec_label,word_label,model,opt,loss_func,context_pred_loss_weight=0.5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    opt.zero_grad()\n",
    "    y_pred, context_pred, attention_vector = model(x1, x2)\n",
    "    rec_loss = loss_func(y_pred,rec_label)\n",
    "    word_loss = loss_func(context_pred,word_label)\n",
    "    total_loss = rec_loss + (context_pred_loss_weight * word_loss)\n",
    "    total_loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    return y_pred, context_pred, attention_vector, rec_loss, word_loss, total_loss\n",
    "\n",
    "def plot_epoch_loss(epoch_losses):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(25,10))\n",
    "    ax.plot(range(len(epoch_losses[0][\"total_loss\"][0])),epoch_losses[0][\"total_loss\"][0],c=\"green\",label=\"Total Loss\")\n",
    "    ax.plot(range(len(epoch_losses[0][\"word_loss\"][0])),epoch_losses[0][\"word_loss\"][0],c=\"blue\",label=\"Word Loss\")\n",
    "    ax.plot(range(len(epoch_losses[0][\"rs_loss\"][0])),epoch_losses[0][\"rs_loss\"][0],c=\"red\",label=\"RS Loss\")\n",
    "    ax.set_xlabel(\"Batch Num\")\n",
    "    ax.set_ylabel(\"Training Loss\")\n",
    "    ax.set_title(\"Training Loss for ATMT\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mocking baseline 2\n",
    "\n",
    "@timer\n",
    "def run_cp(df,cp,doc_2_cluster_map,learning_rate,context_pred_loss_weight,num_epochs,batch_size,increments=10):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    train,test = get_train_test_attm(df,cp,doc_2_cluster_map)\n",
    "        \n",
    "    model, epoch_losses, opt, loss_func = train_model(train,\n",
    "                                                      learning_rate=learning_rate,\n",
    "                                                      context_pred_loss_weight=context_pred_loss_weight,\n",
    "                                                      num_epochs=num_epochs,\n",
    "                                                      batch_size=batch_size)\n",
    "        \n",
    "    inc_losses_test,precisions,recalls = incremental_train(model=model,\n",
    "                                      data=test,\n",
    "                                      opt=opt,\n",
    "                                      loss_func=loss_func,\n",
    "                                      context_pred_loss_weight=context_pred_loss_weight,\n",
    "                                      increments=increments)\n",
    "    \n",
    "    return epoch_losses,inc_losses_test, precisions, recalls\n",
    "\n",
    "@timer\n",
    "def run_all_cps(df,cps,doc_2_cluster_map,learning_rate=0.01,context_pred_loss_weight=0.5,num_epochs=2,batch_size=1,increments=10):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cp_scores = defaultdict(lambda : defaultdict(list))\n",
    "    epoch_train_loss_cps = defaultdict()\n",
    "    inc_test_loss_cps = defaultdict()\n",
    "    \n",
    "    for cp in cps:\n",
    "        print(\"cluster_pair : %s\" %str(cp))\n",
    "        \n",
    "        epoch_losses,inc_losses_test,precisions, recalls = run_cp(df,cp,doc_2_cluster_map,learning_rate,context_pred_loss_weight,num_epochs,batch_size,increments=increments)\n",
    "        epoch_train_loss_cps[cp]= epoch_losses\n",
    "        inc_test_loss_cps[cp] = inc_losses_test\n",
    "        cp_scores[cp][\"precision\"] = precisions\n",
    "        cp_scores[cp][\"recall\"] = recalls\n",
    "    \n",
    "    return epoch_train_loss_cps, inc_test_loss_cps, cp_scores\n",
    "\n",
    "@timer\n",
    "def train_model(data,learning_rate=0.01,context_pred_loss_weight=0.5,num_epochs=2,batch_size=1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cuda1 = torch.device('cuda:1')\n",
    "    model = AttentionMT(embedding_size=768,verbose=True)\n",
    "    model.to(cuda1)\n",
    "    loss_func = nn.BCELoss()\n",
    "    opt = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "    bert_tokenizer = load_tokenizer()\n",
    "    \n",
    "    epoch_losses = defaultdict(lambda : defaultdict(list))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        batch_nums = data.shape[0]/batch_size\n",
    "        batch_total_loss = []\n",
    "        batch_word_loss = []\n",
    "        batch_rs_loss = []\n",
    "        \n",
    "        articles = data[\"text\"].tolist()\n",
    "        context_words = data[\"context_word\"].tolist()\n",
    "        class_labels = data[\"class_label\"].tolist()\n",
    "        word_labels = data[\"word_label\"].tolist()\n",
    "        which_cluster = data[\"which_cluster\"].tolist()\n",
    "        \n",
    "        for batch_num, (article_batch, context_word_batch, class_label_batch, word_label_batch, which_cluster_batch) in enumerate(batch_text_gen(articles,\n",
    "                                                                                                                                                 context_words,\n",
    "                                                                                                                                                 class_labels,\n",
    "                                                                                                                                                 word_labels,\n",
    "                                                                                                                                                 which_cluster,\n",
    "                                                                                                                                                 batch_size=batch_size)):\n",
    "            \n",
    "            bert_tokenized_words, bert_tokenized_word_to_predict, rec_labels, word_labels = tokenize_4bert_batch(article_batch,\n",
    "                                                                                                                 context_word_batch, \n",
    "                                                                                                                 class_label_batch, \n",
    "                                                                                                                 word_label_batch, tokenizer=bert_tokenizer)\n",
    "            opt.zero_grad() # reset all the gradient information\n",
    "    \n",
    "            y_pred, context_pred, attention_vector = model(bert_tokenized_words, bert_tokenized_word_to_predict)\n",
    "            \n",
    "            rec_loss = loss_func(y_pred,rec_labels)\n",
    "            word_loss = loss_func(context_pred,word_labels)\n",
    "            \n",
    "            total_loss = rec_loss + (context_pred_loss_weight * word_loss)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            \n",
    "            batch_rs_loss.append(rec_loss.item())\n",
    "            batch_word_loss.append(word_loss.item())\n",
    "            batch_total_loss.append(total_loss)\n",
    "            \n",
    "            if batch_num % 100 == 0 and batch_num >=100:\n",
    "                print(\"Epoch : %s | Batch : %s | Total Loss : %s | Rec Loss : %s | Word Loss : %s\" % (str(epoch),str(batch_num),str(total_loss.item()),str(rec_loss.item()),str(word_loss.item())))\n",
    "            \n",
    "            \n",
    "        epoch_losses[epoch][\"rs_loss\"].append(batch_rs_loss)\n",
    "        epoch_losses[epoch][\"word_loss\"].append(batch_word_loss)\n",
    "        epoch_losses[epoch][\"total_loss\"].append(batch_total_loss)\n",
    "    \n",
    "    return model, epoch_losses, opt, loss_func\n",
    "\n",
    "@timer\n",
    "def incremental_train(model,data,opt,loss_func,context_pred_loss_weight=0.5,increments=100,pred_batch_size=100):\n",
    "    \"\"\"\n",
    "    * we train with one sample\n",
    "    * recommend from the rest of the candidate pool\n",
    "    * record recommendation\n",
    "    * remove recommendation from candidate pool\n",
    "    * use this recommendation to train again\n",
    "    * repeat for n increments\n",
    "    \"\"\"\n",
    "    \n",
    "    articles = data[\"text\"].tolist()\n",
    "    context_words = data[\"context_word\"].tolist()\n",
    "    class_labels = data[\"class_label\"].tolist()\n",
    "    word_labels = data[\"word_label\"].tolist()\n",
    "        \n",
    "    bert_tokenizer = load_tokenizer()\n",
    "    \n",
    "    bert_tokenized_words, bert_tokenized_word_to_predict, rec_labels, word_labels = tokenize_4bert_batch(articles,\n",
    "                                                                                                         context_words, \n",
    "                                                                                                         class_labels, \n",
    "                                                                                                         word_labels, \n",
    "                                                                                                         tokenizer=bert_tokenizer)\n",
    "    candidate_x1 = bert_tokenized_words\n",
    "    candidate_x2 = bert_tokenized_word_to_predict\n",
    "    candidate_y1 = rec_labels\n",
    "    candidate_y2 = word_labels\n",
    "    \n",
    "    y1_preds = []\n",
    "    y2_preds = []\n",
    "    all_relevant = sum(class_labels)\n",
    "    precisions_k = []\n",
    "    recalls_k = []\n",
    "    \n",
    "    ttl = []\n",
    "    rsl = []\n",
    "    cwl = []\n",
    "    \n",
    "    for i in range(increments):\n",
    "        print(\"Iter : %s\" %str(i))\n",
    "        predicted_probas_y1 = []\n",
    "        predicted_probas_y2 = []\n",
    "        attention_vectors = []\n",
    "        with torch.no_grad(): # eval or recommendation mode\n",
    "#             for i in range(candidate_x1.size(0)):\n",
    "#                 pp_y1, pp_y2, at = model(candidate_x1[i,:].unsqueeze(0),candidate_x2[i,:].unsqueeze(0))\n",
    "#                 predicted_probas_y1.append(pp_y1.unsqueeze(0))\n",
    "#                 predicted_probas_y2.append(pp_y2.unsqueeze(0))\n",
    "#                 attention_vectors.append(at)\n",
    "            \n",
    "            for batch_num, (cp_x1_batch, cp_x2_batch, y1_batch, y2_batch) in enumerate(batch_gen_pred(candidate_x1,candidate_x2,candidate_y1,candidate_y2,batch_size=100)):\n",
    "                pp_y1, pp_y2, at = model(cp_x1_batch,cp_x2_batch)\n",
    "                predicted_probas_y1.append(pp_y1)\n",
    "                predicted_probas_y2.append(pp_y2)\n",
    "                attention_vectors.append(at)\n",
    "        \n",
    "#         predicted_probas_y1 = torch.stack(predicted_probas_y1)\n",
    "#         predicted_probas_y2 = torch.stack(predicted_probas_y2)\n",
    "        \n",
    "        predicted_probas_y1 = torch.cat(predicted_probas_y1,dim=0)\n",
    "        predicted_probas_y2 = torch.cat(predicted_probas_y2,dim=0)\n",
    "        \n",
    "#         attention_vectors = torch.cat(attention_vectors,dim=0)\n",
    "        \n",
    "        # get argmax of predicted_probas_y1\n",
    "        rec_item_ind = torch.topk(predicted_probas_y1, k=1, dim=0)[1].squeeze()\n",
    "        rec_item_y1_label = candidate_y1[rec_item_ind.item()]\n",
    "        \n",
    "        y1_preds.append(rec_item_y1_label.item())\n",
    "        # delete this index from tensors (not trivial as inn numpy)\n",
    "        # need to subsample the tensor instead of deletion \n",
    "        \n",
    "        rec_x1 = candidate_x1[rec_item_ind,:]\n",
    "        word_x2 = candidate_x2[rec_item_ind,:]\n",
    "        rec_label = rec_labels[rec_item_ind]\n",
    "        word_label = word_labels[rec_item_ind]\n",
    "        \n",
    "        candidate_x1 = candidate_x1[torch.arange(candidate_x1.size(0)) != rec_item_ind.cpu(),:]\n",
    "        candidate_x2 = candidate_x2[torch.arange(candidate_x2.size(0)) != rec_item_ind.cpu(),:]\n",
    "        candidate_y1 = candidate_y1[torch.arange(candidate_y1.size(0)) != rec_item_ind.cpu()]\n",
    "        candidate_y2 = candidate_y2[torch.arange(candidate_y2.size(0)) != rec_item_ind.cpu()]\n",
    "        \n",
    "        _,_,_,rec_loss, word_loss, total_loss = partial_train_step(rec_x1.unsqueeze(0),word_x2.unsqueeze(0),rec_label,word_label.unsqueeze(0),model,opt,loss_func,context_pred_loss_weight=context_pred_loss_weight)\n",
    "        \n",
    "        # Recall @K\n",
    "        recall = Counter(y1_preds[:i+1])[1.0]/all_relevant\n",
    "        # Precision @K\n",
    "        precision = (Counter(y1_preds[:i+1])[1.0])/len(y1_preds[:i+1])\n",
    "        \n",
    "        ttl.append(total_loss)\n",
    "        rsl.append(rec_loss)\n",
    "        cwl.append(word_loss)\n",
    "        precisions_k.append(precision)\n",
    "        recalls_k.append(recall)\n",
    "    \n",
    "    # calculate metrics \n",
    "    losses = defaultdict()\n",
    "    losses[\"total_loss\"] = ttl\n",
    "    losses[\"rec_loss\"] = rsl\n",
    "    losses[\"word_loss\"] = cwl\n",
    "    \n",
    "    return losses, precisions_k, recalls_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding IDF threshold by checking distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_data(file_path = data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_idf = get_vocab(df[\"processed_title\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(8,8))\n",
    "# sns.histplot(list(vocab_idf.values()),ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Using Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"context_word_pos\"] = sample_context_words(text_list=df[\"processed_title\"],\n",
    "#                                               idf_thresh=9.0,\n",
    "#                                               pos_sample_num=3,\n",
    "#                                               neg_sample_num=3,\n",
    "#                                               cword_type=\"pos\",\n",
    "#                                               filter_word_pieces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"context_word_neg\"] = sample_context_words(text_list=df[\"processed_title\"],\n",
    "#                                               idf_thresh=9.0,\n",
    "#                                               neg_sample_num=3,\n",
    "#                                               cword_type=\"neg\",\n",
    "#                                               filter_word_pieces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = load_bert_embeddings(df=df,\n",
    "#                                 saved_path=\"/media/karthikshivaram/SABER_4TB/bert_embeddings\",\n",
    "#                                 batch_size=50,\n",
    "#                                 layer=12,\n",
    "#                                 context_var=100,\n",
    "#                                 aggregation=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df , doc_2_cluster_map, top100 = get_cluster_pairs_top(df,vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # clean_df.to_csv(\"temp_df.csv\",index=False)\n",
    "# with open(\"clean_df.pickle\",'wb') as handle:\n",
    "#     pickle.dump(clean_df,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('d2c_map.pickle', 'wb') as handle:\n",
    "#     pickle.dump(doc_2_cluster_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('top100.pickle', 'wb') as handle:\n",
    "#     pickle.dump(top100, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import ast\n",
    "clean_df = None\n",
    "doc_2_cluster_map = None\n",
    "top100 = None\n",
    "with open(\"clean_df.pickle\",'rb') as handle:\n",
    "    clean_df = pickle.load(handle)\n",
    "with open('d2c_map.pickle', 'rb') as handle:\n",
    "    doc_2_cluster_map = pickle.load(handle)\n",
    "with open('top100.pickle', 'rb') as handle:\n",
    "    top100 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source_partisan_score</th>\n",
       "      <th>binary_ps</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_title</th>\n",
       "      <th>context_word_pos</th>\n",
       "      <th>context_word_neg</th>\n",
       "      <th>drop_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top general warns that 'divisiveness leads to ...</td>\n",
       "      <td>(medianame) America's most senior general warn...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>medianame america most senior general warned ...</td>\n",
       "      <td>top general warns that divisiveness leads to d...</td>\n",
       "      <td>[defeat, grow, leads]</td>\n",
       "      <td>[incredible, ambiguity, clauses]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How the US government created a fake universit...</td>\n",
       "      <td>The Department of Homeland Security created a ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>the department homeland security created fake ...</td>\n",
       "      <td>how the us government created fake university ...</td>\n",
       "      <td>[university, created, DROP_THIS]</td>\n",
       "      <td>[corvette, mold, forge]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's Time To Say It: Trump Is Handling COVID-1...</td>\n",
       "      <td>US President Donald Trump is handling the coro...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>president donald trump handling the coronavir...</td>\n",
       "      <td>its time to say it trump is handling covid19 l...</td>\n",
       "      <td>[dictator, handling, DROP_THIS]</td>\n",
       "      <td>[studying, bows, pits]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump's draconian border lockdown has a new ta...</td>\n",
       "      <td>For the past year, the bridges that cross from...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>for the past year the bridges that cross from ...</td>\n",
       "      <td>trumps draconian border lockdown has new target</td>\n",
       "      <td>[lock, target, DROP_THIS]</td>\n",
       "      <td>[smoked, geneva, detailing]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Supreme Court clears way for execution of fede...</td>\n",
       "      <td>TERRE HAUTE, Ind. (AP)  The Trump administrat...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>terre haute ind the trump administration was m...</td>\n",
       "      <td>supreme court clears way for execution of fede...</td>\n",
       "      <td>[clears, execution, prisoner]</td>\n",
       "      <td>[divisions, rhode, carey]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>After testy call with Trump over border wall, ...</td>\n",
       "      <td>One Mexican official said Trump lost his temp...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>one mexican official said trump lost his tempe...</td>\n",
       "      <td>after testy call with trump over border wall m...</td>\n",
       "      <td>[shelves, mexican, DROP_THIS]</td>\n",
       "      <td>[displays, blooms, inadvertently]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Opinion | If Congress had any pride, it would ...</td>\n",
       "      <td>This pertains to the almost 800,000 dreamers...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>this pertains the almost 800 000 dreamers our ...</td>\n",
       "      <td>opinion if congress had any pride it would set...</td>\n",
       "      <td>[pride, DROP_THIS, DROP_THIS]</td>\n",
       "      <td>[horde, assimilation, abolitionist]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Liz Cheney on Ukraine: 'Starting to Seem Like ...</td>\n",
       "      <td>Rep. Liz Cheney (R-WY) on Monday said an intel...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>rep liz cheney monday said intelligence offici...</td>\n",
       "      <td>liz cheney on ukraine starting to seem like po...</td>\n",
       "      <td>[seem, starting, cheney]</td>\n",
       "      <td>[article, anyone, turkish]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ThinkProgress Smears Dan Crenshaw on Universa...</td>\n",
       "      <td>At ThinkProgress, Josh Israel miscasts Dan Cre...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>thinkprogress josh israel miscasts dan crensh...</td>\n",
       "      <td>thinkprogress smears dan crenshaw on universal...</td>\n",
       "      <td>[universal, background, checks]</td>\n",
       "      <td>[performing, bang, warmly]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pompeo recalls Afghan peace envoy after Trump ...</td>\n",
       "      <td>Secretary of State Mike Pompeo said Sunday tha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>secretary state mike pompeo said sunday that t...</td>\n",
       "      <td>pompeo recalls afghan peace envoy after trump ...</td>\n",
       "      <td>[afghan, envoy, recalls]</td>\n",
       "      <td>[modify, measuring, sitcom]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Top general warns that 'divisiveness leads to ...   \n",
       "1  How the US government created a fake universit...   \n",
       "2  It's Time To Say It: Trump Is Handling COVID-1...   \n",
       "3  Trump's draconian border lockdown has a new ta...   \n",
       "4  Supreme Court clears way for execution of fede...   \n",
       "5  After testy call with Trump over border wall, ...   \n",
       "6  Opinion | If Congress had any pride, it would ...   \n",
       "7  Liz Cheney on Ukraine: 'Starting to Seem Like ...   \n",
       "8  ThinkProgress Smears Dan Crenshaw on Universa...   \n",
       "9  Pompeo recalls Afghan peace envoy after Trump ...   \n",
       "\n",
       "                                                text  source_partisan_score  \\\n",
       "0  (medianame) America's most senior general warn...                   -1.0   \n",
       "1  The Department of Homeland Security created a ...                   -1.0   \n",
       "2  US President Donald Trump is handling the coro...                   -2.0   \n",
       "3  For the past year, the bridges that cross from...                   -2.0   \n",
       "4  TERRE HAUTE, Ind. (AP)  The Trump administrat...                    1.0   \n",
       "5  One Mexican official said Trump lost his temp...                   -1.0   \n",
       "6  This pertains to the almost 800,000 dreamers...                   -1.0   \n",
       "7  Rep. Liz Cheney (R-WY) on Monday said an intel...                    2.0   \n",
       "8  At ThinkProgress, Josh Israel miscasts Dan Cre...                    2.0   \n",
       "9  Secretary of State Mike Pompeo said Sunday tha...                    1.0   \n",
       "\n",
       "   binary_ps                                     processed_text  \\\n",
       "0          0   medianame america most senior general warned ...   \n",
       "1          0  the department homeland security created fake ...   \n",
       "2          0   president donald trump handling the coronavir...   \n",
       "3          0  for the past year the bridges that cross from ...   \n",
       "4          1  terre haute ind the trump administration was m...   \n",
       "5          0  one mexican official said trump lost his tempe...   \n",
       "6          0  this pertains the almost 800 000 dreamers our ...   \n",
       "7          1  rep liz cheney monday said intelligence offici...   \n",
       "8          1   thinkprogress josh israel miscasts dan crensh...   \n",
       "9          1  secretary state mike pompeo said sunday that t...   \n",
       "\n",
       "                                     processed_title  \\\n",
       "0  top general warns that divisiveness leads to d...   \n",
       "1  how the us government created fake university ...   \n",
       "2  its time to say it trump is handling covid19 l...   \n",
       "3    trumps draconian border lockdown has new target   \n",
       "4  supreme court clears way for execution of fede...   \n",
       "5  after testy call with trump over border wall m...   \n",
       "6  opinion if congress had any pride it would set...   \n",
       "7  liz cheney on ukraine starting to seem like po...   \n",
       "8  thinkprogress smears dan crenshaw on universal...   \n",
       "9  pompeo recalls afghan peace envoy after trump ...   \n",
       "\n",
       "                   context_word_pos                     context_word_neg  \\\n",
       "0             [defeat, grow, leads]     [incredible, ambiguity, clauses]   \n",
       "1  [university, created, DROP_THIS]              [corvette, mold, forge]   \n",
       "2   [dictator, handling, DROP_THIS]               [studying, bows, pits]   \n",
       "3         [lock, target, DROP_THIS]          [smoked, geneva, detailing]   \n",
       "4     [clears, execution, prisoner]            [divisions, rhode, carey]   \n",
       "5     [shelves, mexican, DROP_THIS]    [displays, blooms, inadvertently]   \n",
       "6     [pride, DROP_THIS, DROP_THIS]  [horde, assimilation, abolitionist]   \n",
       "7          [seem, starting, cheney]           [article, anyone, turkish]   \n",
       "8   [universal, background, checks]           [performing, bang, warmly]   \n",
       "9          [afghan, envoy, recalls]          [modify, measuring, sitcom]   \n",
       "\n",
       "   drop_  \n",
       "0  False  \n",
       "1  False  \n",
       "2  False  \n",
       "3  False  \n",
       "4  False  \n",
       "5  False  \n",
       "6  False  \n",
       "7  False  \n",
       "8  False  \n",
       "9  False  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_train_loss_cps, inc_test_loss_cps,cp_scores = run_all_cps(clean_df,\n",
    "#                                                   [top100[0]],\n",
    "#                                                   doc_2_cluster_map,\n",
    "#                                                   learning_rate=0.001,\n",
    "#                                                   context_pred_loss_weight=0.5,\n",
    "#                                                   num_epochs=1,\n",
    "#                                                   batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Cluster Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to select cluster pairs that have the most promise to show an effect:\n",
    "\n",
    "For this we want to consider cluster pairs where we have high overlap of the following words:\n",
    "* keywords = [\"democrats\", \"media\",\"socialist\",\"donald\", \"republicans\", \"progressive\"]\n",
    "\n",
    "Strategy 1:\n",
    "-----------\n",
    "* For each cluster pair:\n",
    "* create doc pairs between docs in c1 and c2\n",
    "* find the overlap of the words between each of these doc pairs\n",
    "* rank cps based on thhe overlap values (descending)\n",
    "* repeat for all keywords\n",
    "* take average rank\n",
    "\n",
    "* cant use this as the no of doc pairs is too large\n",
    "\n",
    "Strategy 2:\n",
    "-----------\n",
    "So for each cluster pair:\n",
    "* Find the df for each keyword in each cluster\n",
    "* take the sum across all keywords for each cluster\n",
    "* sum for both clusters\n",
    "* rank by this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def select_cps(cps,cluster_2_doc_map,df,keywords=[\"democrats\", \"media\",\"socialist\",\"donald\", \"republicans\", \"progressive\"]):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    keyword_doc_map = defaultdict(list)\n",
    "    df[\"processed_all\"] = df[\"processed_title\"] + \" \" + df[\"processed_text\"]\n",
    "    articles = df[\"processed_all\"].tolist()\n",
    "    print(len(articles))\n",
    "    a_inds = df.index.values.tolist()\n",
    "    print(len(a_inds))\n",
    "    for a_id,article in zip(a_inds,articles):\n",
    "        for k in keywords:\n",
    "            if k in article:\n",
    "                keyword_doc_map[k].append(a_id)\n",
    "    \n",
    "    print(\"Finished going through all texts\")\n",
    "    overlap_counts = []\n",
    "    for cp in cps:\n",
    "        print(cp)\n",
    "        c1_docs = cluster_2_doc_map[cp[0]]\n",
    "        c2_docs = cluster_2_doc_map[cp[1]]\n",
    "        \n",
    "        doc_pairs =[doc_pair for doc_pair in itertools.product(c1_docs, c2_docs)]\n",
    "        print(len(doc_pairs))\n",
    "        overlap_count = 0\n",
    "        for doc_pair in itertools.product(c1_docs, c2_docs):\n",
    "            for k in keywords:\n",
    "                if doc_pair[0] in keyword_doc_map[k] and doc_pair[0] in keyword_doc_map[k]:\n",
    "                    overlap_count += 1\n",
    "        \n",
    "        overlap_counts.append((cp,overlap_count))\n",
    "    \n",
    "    return sorted(overlap_counts,key=lambda x: x[1],reverse=True)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95120\n",
      "95120\n",
      "Finished going through all texts\n",
      "(46, 56)\n",
      "2015877\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4e50a33748ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcps_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_cps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop100\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster_2_doc_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc_2_cluster_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/combatting_partisanship/Combatting-partisan-homogenization/general_utils.py\u001b[0m in \u001b[0;36mwrapper_timer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-9839d91a6029>\u001b[0m in \u001b[0;36mselect_cps\u001b[0;34m(cps, cluster_2_doc_map, df, keywords)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc_pair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mdoc_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeyword_doc_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdoc_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeyword_doc_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     \u001b[0moverlap_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cps_sorted = select_cps(cps=top100[:10],cluster_2_doc_map=doc_2_cluster_map,df=clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"sorted_cps_imp.pickle\",'wb') as handle:\n",
    "    pickle.dump(cps_sorted,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cps_sorted = None\n",
    "with open('sorted_cps_imp.pickle', 'rb') as handle:\n",
    "    cps_sorted = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_res(scores_):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df_maps = {}\n",
    "    settings = []\n",
    "    score_types = []\n",
    "    output_type = []\n",
    "    f1_scores = []\n",
    "    precis_scores = []\n",
    "    recall_scores = []\n",
    "    auc_roc_scores = []\n",
    "    accuracy_scores = []\n",
    "    for cp in scores_.keys():\n",
    "        for setting in scores_[cp].keys():\n",
    "            for score_type in scores_[cp][setting].keys():\n",
    "                for score_output in scores_[cp][setting][score_type].keys():\n",
    "                    settings.append(setting)\n",
    "                    score_types.append(score_type)\n",
    "                    output_type.append(score_output)\n",
    "                    f1_scores.append(scores_[cp][setting][score_type][score_output][\"f1\"])\n",
    "                    recall_scores.append(scores_[cp][setting][score_type][score_output][\"recall\"])\n",
    "                    precis_scores.append(scores_[cp][setting][score_type][score_output][\"precision\"])\n",
    "                    auc_roc_scores.append(scores_[cp][setting][score_type][score_output][\"roc_auc\"])\n",
    "                    accuracy_scores.append(scores_[cp][setting][score_type][score_output][\"accuracy\"])\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        df[\"Settings\"] = settings\n",
    "        df[\"Score_type\"] = score_types\n",
    "        df[\"Output_type\"] = output_type\n",
    "        df[\"F1\"] = f1_scores\n",
    "        df[\"Precision\"] = precis_scores\n",
    "        df[\"Recall\"] = recall_scores\n",
    "        df[\"ROC_AUC\"] = auc_roc_scores\n",
    "        df[\"Accuracy\"] = accuracy_scores\n",
    "        \n",
    "        df_maps[cp] = df\n",
    "    \n",
    "    return df_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSDA Setting:\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Shape : (1390, 9)\n",
      "Original Test Shape : (1205, 9)\n",
      "Exploded Train Shape : (7672, 4)\n",
      "Exploded Test Shape : (6520, 4)\n",
      "\n",
      "Sample size from C1 in Train : (5370, 5)\n",
      "class_label dist :\n",
      "\n",
      "0    2757\n",
      "1    2613\n",
      "Name: class_label, dtype: int64\n",
      "word_label dist :\n",
      "\n",
      "0    2935\n",
      "1    2435\n",
      "Name: word_label, dtype: int64\n",
      "\n",
      "Sample size from C2 in Train : (537, 5)\n",
      "class_label dist :\n",
      "\n",
      "1    290\n",
      "0    247\n",
      "Name: class_label, dtype: int64\n",
      "word_label dist :\n",
      "\n",
      "0    285\n",
      "1    252\n",
      "Name: word_label, dtype: int64\n",
      "\n",
      "Train Size : (5907, 5)\n",
      "class_label dist :\n",
      "\n",
      "0    3004\n",
      "1    2903\n",
      "Name: class_label, dtype: int64\n",
      "word_label dist :\n",
      "\n",
      "0    3220\n",
      "1    2687\n",
      "Name: word_label, dtype: int64\n",
      "\n",
      "Sample Size from C1 in Test : (2302, 5)\n",
      "class_label dist :\n",
      "\n",
      "0    1230\n",
      "1    1072\n",
      "Name: class_label, dtype: int64\n",
      "word_label dist :\n",
      "\n",
      "0    1235\n",
      "1    1067\n",
      "Name: word_label, dtype: int64\n",
      "\n",
      "Sample Size from C2 in Test : (2302, 5)\n",
      "class_label dist :\n",
      "\n",
      "1    1222\n",
      "0    1080\n",
      "Name: class_label, dtype: int64\n",
      "word_label dist :\n",
      "\n",
      "0    1258\n",
      "1    1044\n",
      "Name: word_label, dtype: int64\n",
      "\n",
      "Test Size : (4604, 5)\n",
      "class_label dist :\n",
      "\n",
      "0    2310\n",
      "1    2294\n",
      "Name: class_label, dtype: int64\n",
      "word_label dist :\n",
      "\n",
      "0    2493\n",
      "1    2111\n",
      "Name: word_label, dtype: int64\n",
      "sleeping for 10\n",
      "Epoch : 0 | Batch : 100 | Total Loss : 0.7031127214431763 | Rec Loss : 0.6997795701026917 | Word Loss : 0.011110474355518818\n",
      "Epoch : 0 | Batch : 200 | Total Loss : 0.9925577640533447 | Rec Loss : 0.6764134168624878 | Word Loss : 1.05381441116333\n",
      "Epoch : 0 | Batch : 300 | Total Loss : 1.4901618957519531 | Rec Loss : 0.7236377000808716 | Word Loss : 2.5550804138183594\n",
      "Epoch : 0 | Batch : 400 | Total Loss : 0.8079535961151123 | Rec Loss : 0.6749205589294434 | Word Loss : 0.44344353675842285\n",
      "Epoch : 0 | Batch : 500 | Total Loss : 1.2792067527770996 | Rec Loss : 0.6822929978370667 | Word Loss : 1.9897122383117676\n",
      "Epoch : 0 | Batch : 600 | Total Loss : 0.8608989119529724 | Rec Loss : 0.7035112380981445 | Word Loss : 0.5246255397796631\n",
      "Epoch : 0 | Batch : 700 | Total Loss : 0.960640013217926 | Rec Loss : 0.6869581341743469 | Word Loss : 0.9122728705406189\n",
      "Epoch : 0 | Batch : 800 | Total Loss : 0.7905310988426208 | Rec Loss : 0.6933135390281677 | Word Loss : 0.3240585923194885\n",
      "Epoch : 0 | Batch : 900 | Total Loss : 0.9593703746795654 | Rec Loss : 0.6937134265899658 | Word Loss : 0.8855231404304504\n",
      "Epoch : 0 | Batch : 1000 | Total Loss : 0.7597526907920837 | Rec Loss : 0.6803503036499023 | Word Loss : 0.26467466354370117\n",
      "Epoch : 0 | Batch : 1100 | Total Loss : 0.9817924499511719 | Rec Loss : 0.7212488651275635 | Word Loss : 0.868478536605835\n",
      "Epoch : 0 | Batch : 1200 | Total Loss : 1.322098731994629 | Rec Loss : 0.7025991082191467 | Word Loss : 2.0649983882904053\n",
      "Epoch : 0 | Batch : 1300 | Total Loss : 1.1263211965560913 | Rec Loss : 0.6975163817405701 | Word Loss : 1.4293493032455444\n",
      "Epoch : 0 | Batch : 1400 | Total Loss : 0.9561724066734314 | Rec Loss : 0.710198163986206 | Word Loss : 0.8199141025543213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karthikshivaram/anaconda3/envs/frames/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/karthikshivaram/anaconda3/envs/frames/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/karthikshivaram/anaconda3/envs/frames/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished running 'batch_predict_and_score' in 4.6525 mins\n",
      "\n",
      "\n",
      "Finished running 'train_ssda' in 21.6515 mins\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karthikshivaram/anaconda3/envs/frames/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished running 'batch_predict_and_score' in 3.5915 mins\n",
      "\n",
      "Original Train Shape : (1479, 9)\n",
      "Original Test Shape : (1303, 9)\n",
      "Exploded Train Shape : (7981, 4)\n",
      "Exploded Test Shape : (6933, 4)\n",
      "\n",
      "Sample size from C1 in Train : (5586, 5)\n",
      "class_label dist :\n",
      "\n",
      "1    2954\n",
      "0    2632\n",
      "Name: class_label, dtype: int64\n",
      "word_label dist :\n",
      "\n",
      "0    3136\n",
      "1    2450\n",
      "Name: word_label, dtype: int64\n",
      "\n",
      "Sample size from C2 in Train : (558, 5)\n",
      "class_label dist :\n",
      "\n",
      "1    315\n",
      "0    243\n",
      "Name: class_label, dtype: int64\n",
      "word_label dist :\n",
      "\n",
      "0    323\n",
      "1    235\n",
      "Name: word_label, dtype: int64\n",
      "\n",
      "Train Size : (6144, 5)\n",
      "class_label dist :\n",
      "\n",
      "1    3269\n",
      "0    2875\n",
      "Name: class_label, dtype: int64\n",
      "word_label dist :\n",
      "\n",
      "0    3459\n",
      "1    2685\n",
      "Name: word_label, dtype: int64\n",
      "\n",
      "Sample Size from C1 in Test : (2395, 5)\n",
      "class_label dist :\n",
      "\n",
      "1    1248\n",
      "0    1147\n",
      "Name: class_label, dtype: int64\n",
      "word_label dist :\n",
      "\n",
      "0    1301\n",
      "1    1094\n",
      "Name: word_label, dtype: int64\n",
      "\n",
      "Sample Size from C2 in Test : (2395, 5)\n",
      "class_label dist :\n",
      "\n",
      "1    1382\n",
      "0    1013\n",
      "Name: class_label, dtype: int64\n",
      "word_label dist :\n",
      "\n",
      "0    1325\n",
      "1    1070\n",
      "Name: word_label, dtype: int64\n",
      "\n",
      "Test Size : (4790, 5)\n",
      "class_label dist :\n",
      "\n",
      "1    2630\n",
      "0    2160\n",
      "Name: class_label, dtype: int64\n",
      "word_label dist :\n",
      "\n",
      "0    2626\n",
      "1    2164\n",
      "Name: word_label, dtype: int64\n",
      "sleeping for 10\n",
      "Epoch : 0 | Batch : 100 | Total Loss : 0.9373183250427246 | Rec Loss : 0.6904674768447876 | Word Loss : 0.82283616065979\n",
      "Epoch : 0 | Batch : 200 | Total Loss : 0.9467039108276367 | Rec Loss : 0.688347578048706 | Word Loss : 0.8611878156661987\n",
      "Epoch : 0 | Batch : 300 | Total Loss : 0.8200339078903198 | Rec Loss : 0.800826907157898 | Word Loss : 0.06402338296175003\n",
      "Epoch : 0 | Batch : 400 | Total Loss : 1.210542917251587 | Rec Loss : 0.6965434551239014 | Word Loss : 1.713331699371338\n",
      "Epoch : 0 | Batch : 500 | Total Loss : 0.9485271573066711 | Rec Loss : 0.7770839929580688 | Word Loss : 0.5714771747589111\n",
      "Epoch : 0 | Batch : 600 | Total Loss : 0.9218114614486694 | Rec Loss : 0.6909908056259155 | Word Loss : 0.769402265548706\n",
      "Epoch : 0 | Batch : 700 | Total Loss : 0.8518643379211426 | Rec Loss : 0.6425703763961792 | Word Loss : 0.6976465582847595\n",
      "Epoch : 0 | Batch : 800 | Total Loss : 1.111960530281067 | Rec Loss : 0.7028698921203613 | Word Loss : 1.3636354207992554\n",
      "Epoch : 0 | Batch : 900 | Total Loss : 0.891283392906189 | Rec Loss : 0.6942745447158813 | Word Loss : 0.6566962003707886\n",
      "Epoch : 0 | Batch : 1000 | Total Loss : 0.8824332356452942 | Rec Loss : 0.6812325716018677 | Word Loss : 0.6706687808036804\n",
      "Epoch : 0 | Batch : 1100 | Total Loss : 0.8677893877029419 | Rec Loss : 0.691591203212738 | Word Loss : 0.587327241897583\n",
      "Epoch : 0 | Batch : 1200 | Total Loss : 0.8762608766555786 | Rec Loss : 0.6821290254592896 | Word Loss : 0.6471061110496521\n",
      "Epoch : 0 | Batch : 1300 | Total Loss : 0.974337100982666 | Rec Loss : 0.7174637317657471 | Word Loss : 0.8562445640563965\n",
      "Epoch : 0 | Batch : 1400 | Total Loss : 1.0492558479309082 | Rec Loss : 0.8047786355018616 | Word Loss : 0.814923882484436\n",
      "Epoch : 0 | Batch : 1500 | Total Loss : 1.064681887626648 | Rec Loss : 0.6975302696228027 | Word Loss : 1.2238388061523438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karthikshivaram/anaconda3/envs/frames/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/karthikshivaram/anaconda3/envs/frames/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/karthikshivaram/anaconda3/envs/frames/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished running 'batch_predict_and_score' in 4.8639 mins\n",
      "\n",
      "\n",
      "Finished running 'train_ssda' in 22.4866 mins\n",
      "\n",
      "\n",
      "Finished running 'batch_predict_and_score' in 3.7473 mins\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karthikshivaram/anaconda3/envs/frames/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/karthikshivaram/anaconda3/envs/frames/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/karthikshivaram/anaconda3/envs/frames/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cp_test_scores, cp_train_scores, cp_train_losses = run_ssda_all(df=clean_df,\n",
    "                                                                cps=[top100[4], top100[6]],\n",
    "                                                                doc_2_cluster_map=doc_2_cluster_map,\n",
    "                                                                learning_rates=[0.00001],\n",
    "                                                                epochs=1,\n",
    "                                                                word_pred_loss_weights=[0.3],\n",
    "                                                                batch_size=4,\n",
    "                                                                neg_sample_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(26, 30), (56, 57)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_train_losses.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, (1e-05, 0.3)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_train_losses[(26,30)].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp_test_scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp_test_scores[(46,56)].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp_test_scores[(46,56)][(0.01,0.8)].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_maps = print_res(cp_train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_maps = print_res(cp_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_score_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_scores.pickle\",'wb') as handle:\n",
    "    pickle.dump(test_score_maps,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_scores.pickle\",\"wb\") as handle:\n",
    "    pickle.dump(train_score_maps,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(26, 30), (56, 57)])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_train_losses.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, (0.0001, 0.8)])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_train_losses[(26,30)].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['rs_loss', 'word_loss', 'total_loss'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_train_losses[(26,30)][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epoch_loss(epoch_losses_):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_losses = epoch_losses_\n",
    "    fig,ax = plt.subplots(nrows=1,ncols=1,figsize=(25,10))\n",
    "    ax.plot(range(len(epoch_losses[0][\"total_loss\"][0])),epoch_losses[0][\"total_loss\"][0],c=\"green\",label=\"Total Loss\")\n",
    "    ax.plot(range(len(epoch_losses[0][\"word_loss\"][0])),epoch_losses[0][\"word_loss\"][0],c=\"blue\",label=\"Word Loss\")\n",
    "    ax.plot(range(len(epoch_losses[0][\"rs_loss\"][0])),epoch_losses[0][\"rs_loss\"][0],c=\"red\",label=\"RS Loss\")\n",
    "    ax.set_xlabel(\"Batch Num\")\n",
    "    ax.set_ylabel(\"Training Loss\")\n",
    "    ax.set_title(\"Training Loss for ATMT\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#     plt.savefig('att_loss_figs/%s.png'%str(cp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_epoch_loss(cp_train_losses[(26,30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cp in test_score_maps.keys():\n",
    "    df = test_score_maps[cp]\n",
    "    df.to_csv(\"att_test_res/%s.csv\"%str(cp),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Frames",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
