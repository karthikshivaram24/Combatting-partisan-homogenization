{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter,defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import scale\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from functools import wraps\n",
    "from time import time\n",
    "import more_itertools\n",
    "from functools import partial\n",
    "import functools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 16\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_Seed Chosen : 15112\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(24101990)\n",
    "RANDOM_SEED = np.random.randint(low=1, high=100000, size=1)[0]\n",
    "print(\"Random_Seed Chosen : %s\" %str(RANDOM_SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    \"\"\"\n",
    "    Decorator to time a given function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : generic\n",
    "        The function to time\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    No Exceptions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    value : generic\n",
    "        The return value from func\n",
    "\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_timer(*args,**kwargs):\n",
    "        start = time.perf_counter()\n",
    "        value = func(*args,**kwargs)\n",
    "        stop = time.perf_counter()\n",
    "        run_time = stop - start\n",
    "        print(f\"\\nFinished running {func.__name__!r} in {run_time/60.0:.4f} mins\\n\")\n",
    "        return value\n",
    "    return wrapper_timer\n",
    "\n",
    "@timer\n",
    "def preprocess_texts(text_lists):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def select_first10(x):\n",
    "        return \" \".join(sent_tokenize(x)[:10])\n",
    "    \n",
    "    def to_lower(x):\n",
    "        return x.lower()\n",
    "    \n",
    "    def remove_punc(x):\n",
    "        return re.sub(r'[^\\w\\s]', '  ', x)\n",
    "    \n",
    "    def remove_small_words(x):\n",
    "        return re.sub(r'\\b\\w{1,2}\\b', '', x)\n",
    "    \n",
    "    def remove_spaces(x):\n",
    "        return re.sub(' +', ' ', x)\n",
    "    \n",
    "    preprocess_pipe = [select_first10,to_lower,remove_punc,remove_small_words,remove_spaces]\n",
    "    \n",
    "    processed_texts = text_lists\n",
    "    for preprocess_func in preprocess_pipe:\n",
    "        print(\"Running : %s\" %str(preprocess_func.__name__))\n",
    "        processed_texts = Parallel(n_jobs=-1)(delayed(preprocess_func)(x) for x in processed_texts)\n",
    "        \n",
    "    \n",
    "#     texts_list = list(map(lambda x: x.lower(),texts_list))\n",
    "#     # remove punctuations\n",
    "#     texts_list = list(map(lambda x: re.sub(r'[^\\w\\s]', '  ', x),texts_list))\n",
    "#     # remove 1,2 character words\n",
    "#     texts_list = list(map(lambda x: re.sub(r'\\b\\w{1,2}\\b', '', x),texts_list))\n",
    "#     # remove multiple spaces\n",
    "#     texts_list = list(map(lambda x: re.sub(' +', ' ', x),texts_list))\n",
    "    return processed_texts\n",
    "\n",
    "@timer\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    print(df.columns)\n",
    "    print(\"Df original shape : %s\" %str(df.shape))\n",
    "    # drop rows with text as nan\n",
    "    df = df[df['text'].notna()]\n",
    "    print(\"Df shape after dropping nan text : %s\" %str(df.shape))\n",
    "    # drop articles that have stance = 0\n",
    "    df = df[df[\"source_partisan_score\"] != 0]\n",
    "    print(\"Df shape after dropping 0 stance articles : %s\" %str(df.shape))\n",
    "    # convert articles of stance -1,+1,-2,+2\n",
    "    df[\"binary_ps\"] = df[\"source_partisan_score\"].apply(lambda x: 1 if x>=1 else 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "@timer\n",
    "def sample_data(df,sample_size=0.2,seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return df.sample(frac=sample_size, replace=False, random_state=seed)\n",
    "\n",
    "@timer\n",
    "def vectorization(df,min_df=30,max_df=0.75,seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df[\"all_text\"] = df[\"title\"] + \" \" + df[\"processed_text\"]\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=min_df, binary=False, max_df=max_df, stop_words='english',max_features=15000)\n",
    "    vectors = tfidf_vectorizer.fit_transform(df[\"all_text\"])\n",
    "    vocab = tfidf_vectorizer.vocabulary_\n",
    "    print(\"vocab_size : %s\"%str(len(vocab)))\n",
    "    return vectors,vocab,tfidf_vectorizer\n",
    "\n",
    "@timer\n",
    "def dimensionality_reduction(vectors,mode=\"PCA\",dim=500,seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"\\nShape Before DIM REDUC : %s\" %str(vectors.shape))\n",
    "    if mode == \"PCA\":\n",
    "        pca = PCA(n_components=dim,svd_solver=\"arpack\",random_state=seed)\n",
    "        vectors = pca.fit_transform(vectors)\n",
    "        print(\"Shape After DIM REDUC : %s\" %str(vectors.shape))\n",
    "        return vectors\n",
    "    \n",
    "    if mode == \"UMAP\":\n",
    "        return None\n",
    "\n",
    "@timer\n",
    "def run_clustering(vectors,seed=RANDOM_SEED,num_clusters=1000,clus_type=\"kmeans\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if clus_type == \"kmeans\":\n",
    "        print(\"\\nRunning KMEANS Clustering with k=%s\" %str(num_clusters))\n",
    "        km = MiniBatchKMeans(n_clusters=num_clusters, random_state=seed, n_init=3, max_iter=200, batch_size=1000)\n",
    "        clusters = km.fit_predict(vectors)\n",
    "        return clusters,km\n",
    "    \n",
    "    if clus_type == \"spectral\":\n",
    "        return None\n",
    "    \n",
    "    if clus_type == \"dbscan\":\n",
    "        return None\n",
    "\n",
    "@timer\n",
    "def get_cluster_sizes(cluster_clf):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cluster_sizes = Counter(cluster_clf.labels_)\n",
    "    return cluster_sizes\n",
    "\n",
    "@timer\n",
    "def score_cluster(vectors,cluster_clf,score_type=\"sil_score\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if score_type == \"sil_score\":\n",
    "        sil_score = metrics.silhouette_score(vectors, cluster_clf.labels_, metric='euclidean')\n",
    "        print(\"\\nSilhouetter Score : %s\" %str(sil_score))\n",
    "        return sil_score\n",
    "    \n",
    "    return None\n",
    "\n",
    "@timer\n",
    "def get_cluster_pairs(num_clusters):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cluster_pairs = [(a,b) for a,b in more_itertools.pairwise([i for i in range(num_clusters)])]\n",
    "    print(\"\\nNumber of Cluster Pairs : %s\" %str(len(cluster_pairs)))\n",
    "\n",
    "@timer\n",
    "def get_pairwise_dist(cluster_clf,dist_type=\"cosine\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    pairwise_dist = None\n",
    "    if dist_type == \"cosine\":\n",
    "        pairwise_dist = cosine_similarity(cluster_clf.cluster_centers_)\n",
    "    return pairwise_dist\n",
    "\n",
    "@timer\n",
    "def cluster2doc(num_texts,cluster_labels):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cluster_2_doc = defaultdict(list)\n",
    "    for index in range(num_texts):\n",
    "        cluster = cluster_labels[index]\n",
    "        cluster_2_doc[cluster].append(index)\n",
    "    return cluster_2_doc\n",
    "\n",
    "\n",
    "@timer\n",
    "def filter_clusters(cluster_pairs,\n",
    "                    doc_2_cluster_map,\n",
    "                    cluster_sizes,\n",
    "                    partisan_scores,\n",
    "                    min_size=300,\n",
    "                    max_size=5000,\n",
    "                    min_partisan_size=0.3):\n",
    "    \"\"\"\n",
    "    min_partisan_size : percentage of docs in the cluster that must have partisan score of 0 (and similarly 1), this removes pure clusters as well\n",
    "    \"\"\"\n",
    "\n",
    "    def get_cluster_partisan_map(doc_2_cluster_map,partisan_scores):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        cluster_partisan_map = defaultdict(int)\n",
    "        for cluster in doc_2_cluster_map:\n",
    "            ps_scores = []\n",
    "            for doc_id in doc_2_cluster_map[cluster]:\n",
    "                ps_scores.append(partisan_scores[i])\n",
    "            cluster_partisan_map[cluster]=ps_scores\n",
    "        \n",
    "        return cluster_partisan_map\n",
    "    \n",
    "    cluster_partisan_map = get_cluster_partisan_map(doc_2_cluster_map,partisan_scores)\n",
    "    \n",
    "    def filter_min_max(cluster_pair,cluster_sizes):\n",
    "        \"\"\"\n",
    "        Boolean Func\n",
    "        \"\"\"\n",
    "        verdict = True\n",
    "        cond1 = cluster_sizes[cluster_pair[0]] >= min_size and cluster_sizes[cluster_pair[0]] <= max_size \n",
    "        cond2 = cluster_sizes[cluster_pair[1]] >= min_size and cluster_sizes[cluster_pair[1]] <= max_size \n",
    "        if cond1 == True and cond2 == True:\n",
    "            verdict = False\n",
    "        \n",
    "        return verdict\n",
    "    \n",
    "    partial_filter_min_max = partial(filter_min_max(cluster_pair,\n",
    "                                                    cluster_sizes=cluster_sizes))\n",
    "    \n",
    "    def filter_partisan_size(cluster_pair,min_partisan_size,cluster_sizes,cluster_partisan_map):\n",
    "        \"\"\"\n",
    "        Boolean Func\n",
    "        takes a cluster pair and checks the partisan distribution compaired to its cluster size\n",
    "        \n",
    "        \"\"\"\n",
    "        conds = [True,True]\n",
    "        for i,c in enumerate(cluster_pair):\n",
    "            cluster_partisan = cluster_partisan_map[c]\n",
    "            partisan_size = Counter(cluster_partisans)\n",
    "            if partisan_size[0] >= int(cluster_sizes[c]*min_partisan_size) and partisan_size[1] >= int(cluster_sizes[c]*min_partisan_size):\n",
    "                conds[i] = False\n",
    "        \n",
    "        if conds[0] == conds[1] == False:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    \n",
    "    partial_filter_partisan_size = partial(filter_partisan_size(cluster_pair,\n",
    "                                                                min_partisan_size=min_partisan_size,\n",
    "                                                                cluster_sizes=cluster_sizes,\n",
    "                                                                cluster_partisan_map=cluster_partisan_map))\n",
    "    \n",
    "    filter_verdicts_min_max = Parallel(n_jobs=-1)(delayed(partial_filter_min_max)(c_p) for c_p in cluster_pairs)\n",
    "    filter_verdicts_partisan_size = Parallel(n_jobs=-1)(delayed(partial_filter_partisan_size)(c_p) for c_p in cluster_pairs)\n",
    "    \n",
    "    filtered_cps = []\n",
    "    for index,cp in enumerate(cluster_pairs):\n",
    "        if not filter_verdicts_min_max[index] and not filter_verdicts_partisan_size[index]:\n",
    "            filtered_cps.append(cp)\n",
    "    \n",
    "    return filtered_cps\n",
    "\n",
    "@timer\n",
    "def get_top_100_clusterpairs(cluster_pairs,dist_matrix,reverse=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    sorted_cps = sorted(cluster_pairs,key=lambda x: dist_matrix[x[0],x[1]],reverse=reverse)[:100]\n",
    "    return sorted_cps\n",
    "    \n",
    "\n",
    "@timer\n",
    "def create_train_test(cluster_pair,doc_2_cluster_map,X_feats,df,user_type=\"Heterogeneous\"):\n",
    "    \"\"\"\n",
    "    Labels are based on conservative when homogenous\n",
    "    \"\"\"\n",
    "    c1 = cluster_pair[0]\n",
    "    c2 = cluster_pair[1]\n",
    "    \n",
    "    x_train = X_feats[cluster2doc[c1]]\n",
    "    x_test = X_feats[cluster2doc[c2]]\n",
    "    \n",
    "    ps_train = df[\"binary_ps\"].values[cluster2doc[c1]]\n",
    "    ps_test = df[\"binary_ps\"].values[cluster2doc[c2]]\n",
    "    \n",
    "    if user_type == \"Heterogeneous\":\n",
    "        y_train = ps_train\n",
    "        y_test = list(map(lambda x: 0.0 if x==1.0 else 1.0,ps_test))\n",
    "    \n",
    "    if user_type == \"Homogeneous\":\n",
    "        y_train = ps_train\n",
    "        y_test = ps_test\n",
    "    \n",
    "    u_train,c_train = np.unique(y_train, return_counts=True)\n",
    "    u_test,c_test = np.unique(y_test, return_counts=True)\n",
    "    print(\"Train Label Dist :\\n %s : %s\\n %s:%s\" %(str(u_train[0]),str(c_train[0]),str(u_train[1]),str(c_train[1])))\n",
    "    print(\"Test Label Dist :\\n %s : %s\\n %s:%s\" %(str(u_test[0]),str(c_test[0]),str(u_test[1]),str(c_test[1])))\n",
    "    \n",
    "    assert x_train.shape[0] == len(y_train)\n",
    "    assert x_test.shape[0] == len(y_test)\n",
    "    \n",
    "    return x_train,x_test,y_train,y_test\n",
    "\n",
    "\n",
    "def get_scores(y_test,predictions,threshold):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    predicted_probas = predictions[:,1]\n",
    "    predictions = np.where(predicted_probas>=threshold,1,0).flatten()\n",
    "    f1 = metrics.f1_score(y_test,predictions,zero_division=0,average=\"macro\")\n",
    "    precision = metrics.precision_score(y_test,predictions,zero_division=0)\n",
    "    recall = metrics.recall_score(y_test,predictions,zero_division=0)\n",
    "    accuracy = metrics.accuracy_score(y_test,predictions)\n",
    "    \n",
    "    return f1,precision,recall,accuracy\n",
    "\n",
    "@timer\n",
    "def run_model(x_train,x_test,y_train,y_test,seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    clf = LogisticRegressionCV(cv=5,random_state=seed,max_iter=1000,n_jobs=-1,class_weight=\"balanced\").fit(x_train, y_train)\n",
    "    predicted_probas = clf.predict_proba(x_test)\n",
    "    print(predicted_probas.shape)\n",
    "    return clf,predicted_probas\n",
    "\n",
    "\n",
    "def plot_size_dist(cluster_sizes):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.bar(cluster_sizes.keys(), cluster_sizes.values(),width=2)\n",
    "    plt.xlim([0,len(list(cluster_sizes.keys()))])\n",
    "    plt.xlabel(\"Cluster-Number\")\n",
    "    plt.ylim([0,6000])\n",
    "    plt.ylabel(\"Documents in Cluster\")\n",
    "    plt.title(\"Cluster Size Distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "@timer\n",
    "def plot_sim_vs_score(df_results,threshold):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    fig,ax = plt.subplots(nrows=2,ncols=2,figsize=(15,15))\n",
    "    axes = ax.ravel()\n",
    "    sim_score = df_results[\"Cosine Distance\"].tolist()\n",
    "    scores = df_results[\"threshold - %s\"%str(threshold)].tolist()\n",
    "    f1,precision,recall,accuracy = zip(*scores)\n",
    "    \n",
    "    s = 50\n",
    "    \n",
    "    axes[0].scatter(sim_score,f1,marker=\"*\",s=s)\n",
    "    z = np.polyfit(sim_score,f1, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[0].plot(sim_score,p(sim_score),\"r--\")\n",
    "    axes[0].set_xlabel(\"Cosine Similarity between cluster pairs\")\n",
    "    axes[0].set_ylabel(\"F1-Score\")\n",
    "    \n",
    "    axes[1].scatter(sim_score,precision,marker=\"o\",s=s)\n",
    "    z = np.polyfit(sim_score,precision, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[1].plot(sim_score,p(sim_score),\"r--\")\n",
    "    axes[1].set_xlabel(\"Cosine Similarity between cluster pairs\")\n",
    "    axes[1].set_ylabel(\"Precision\")\n",
    "    \n",
    "    axes[2].scatter(sim_score,recall,marker=\"^\",s=s)\n",
    "    z = np.polyfit(sim_score,recall, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[2].plot(sim_score,p(sim_score),\"r--\")\n",
    "    axes[2].set_xlabel(\"Cosine Similarity between cluster pairs\")\n",
    "    axes[2].set_ylabel(\"Recall\")\n",
    "    \n",
    "    axes[3].scatter(sim_score,accuracy,marker=\"+\",s=s)\n",
    "    z = np.polyfit(sim_score,accuracy, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[3].plot(sim_score,p(sim_score),\"r--\")\n",
    "    axes[3].set_xlabel(\"Cosine Similarity between cluster pairs\")\n",
    "    axes[3].set_ylabel(\"Accuracy\")\n",
    "    \n",
    "    fig.suptitle(\"Cluster Similarity vs Classifier Performance | Threshold : %s\" %str(threshold))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "@timer\n",
    "def run_train_all(X,cluster_2_doc_map,df,cluster_pairs,cosine_mat,thresholds = [0.5,0.7,0.9],user_type=\"Heterogeneous\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    results = defaultdict(list)\n",
    "    for index,cp in enumerate(cluster_pairs):\n",
    "            print(\"Training model for cluster pair : %s\" %str(index))\n",
    "            x_train,x_test,y_train,y_test = create_train_test(cluster_pair=cp,\n",
    "                                                              cluster2doc=cluster_2_doc_map,\n",
    "                                                              X_feats=X,\n",
    "                                                              df=df,\n",
    "                                                              user_type=user_type)\n",
    "\n",
    "            clf,predicted_probas = run_model(x_train,x_test,y_train,y_test)\n",
    "\n",
    "            for t in thresholds:\n",
    "                f1,precision,recall,accuracy = get_scores(y_test,\n",
    "                                                          predictions=predicted_probas,\n",
    "                                                          threshold=t)\n",
    "                results[t].append((f1,precision,recall,accuracy))\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_results = pd.DataFrame(cluster_pairs,columns=[\"Cluster1\",\"Cluster2\"])\n",
    "    df_results[\"Cosine Sim\"] = df.apply(lambda x: cosine_mat[x[0],x[1]])\n",
    "    for k in results:\n",
    "        df_results[\"threshold - %s\"%str(k)] = results[k]\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Components\n",
    "\n",
    "1) Data Loading  \n",
    "2) Sampling  \n",
    "3) Text Preprocessing  \n",
    "4) Vectorization  \n",
    "5) Dimensionality Reductions  \n",
    "6) Clustering  \n",
    "7) Clustering Performance Check  \n",
    "8) Cluster Filtering  \n",
    "9) Pairwise Selection  \n",
    "10) Model Training and Performance  \n",
    "11) Pairwise score analysis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['article_id', 'url', 'title', 'text', 'source', 'source_partisan_score',\n",
      "       'tweet_id', 'tweet_screen_name', 'tweet_created_at', 'tweet_text'],\n",
      "      dtype='object')\n",
      "Df original shape : (921037, 10)\n",
      "Df shape after dropping nan text : (919430, 10)\n",
      "Df shape after dropping 0 stance articles : (636722, 10)\n",
      "\n",
      "Finished running 'load_data' in 0.6096 mins\n",
      "\n",
      "\n",
      "Finished running 'sample_data' in 0.0041 mins\n",
      "\n",
      "Sampled Size: 127344\n",
      "Running : select_first10\n",
      "Running : to_lower\n",
      "Running : remove_punc\n",
      "Running : remove_small_words\n",
      "Running : remove_spaces\n",
      "\n",
      "Finished running 'preprocess_texts' in 0.7602 mins\n",
      "\n",
      "vocab_size : 15000\n",
      "\n",
      "Finished running 'vectorization' in 0.3500 mins\n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 14.2 GiB for an array with shape (127344, 15000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f6fde69c94d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampled_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mreduced_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdimensionality_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"PCA\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduced_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclus_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"kmeans\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/1/karthik/anaconda3/envs/upp/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mtodense\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \"\"\"\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/1/karthik/anaconda3/envs/upp/lib/python3.8/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/1/karthik/anaconda3/envs/upp/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 14.2 GiB for an array with shape (127344, 15000) and data type float64"
     ]
    }
   ],
   "source": [
    "# Pipeline test\n",
    "path = \"/data/3/karthik/preference_prediction/articles.csv\"\n",
    "\n",
    "# Pipeline test\n",
    "path = \"~/articles.csv\"\n",
    "\n",
    "main_df = load_data(path)\n",
    "\n",
    "sampled_df = sample_data(df=main_df,sample_size=0.05,seed=RANDOM_SEED)\n",
    "print(\"Sampled Size: %s\" %str(sampled_df.shape[0]))\n",
    "\n",
    "sampled_df[\"processed_text\"] = preprocess_texts(text_lists=sampled_df[\"text\"])\n",
    "\n",
    "vectors,vocab,tfidf_vectorizer = vectorization(df=sampled_df,min_df=50,max_df=0.75,seed=RANDOM_SEED)\n",
    "\n",
    "reduced_vectors = dimensionality_reduction(vectors=vectors.todense(),mode=\"PCA\",dim=500,seed=RANDOM_SEED)\n",
    "\n",
    "clusters,cluster_clf = run_clustering(vectors=reduced_vectors,seed=RANDOM_SEED,num_clusters=1000,clus_type=\"kmeans\")\n",
    "\n",
    "cluster_score = score_cluster(vectors,cluster_clf,score_type=\"sil_score\")\n",
    "\n",
    "cluster_sizes = get_cluster_sizes(cluster_clf)\n",
    "\n",
    "plot_size_dist(cluster_sizes)\n",
    "\n",
    "cluster_pair_dist_mat = get_pairwise_dist(cluster_clf,dist_type=\"cosine\")\n",
    "\n",
    "cluster_pairs = get_cluster_pairs(num_clusters=500)\n",
    "\n",
    "doc_2_cluster_map = cluster2doc(num_texts=sampled_df.shape[0],cluster_labels=cluster_clf.labels_)\n",
    "\n",
    "filtered_cluster_pairs = filter_clusters(cluster_pairs=cluster_pairs,\n",
    "                                        doc_2_cluster_map=doc_2_cluster_map,\n",
    "                                        cluster_sizes=cluster_sizes,\n",
    "                                        partisan_scores=sampled_df[\"binary_ps\"].tolist(),\n",
    "                                        min_size=300,\n",
    "                                        max_size=5000,\n",
    "                                        min_partisan_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = run_train_all(X = reduced_vectors,\n",
    "                          cluster_2_doc_map = doc_2_cluster_map ,\n",
    "                          df = sampled_df ,\n",
    "                          cluster_pairs = filtered_cluster_pairs,\n",
    "                          cosine_mat = cluster_pair_dist_mat,\n",
    "                          thresholds = [0.5,0.7,0.9],\n",
    "                          user_type=\"Heterogeneous\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upp",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
